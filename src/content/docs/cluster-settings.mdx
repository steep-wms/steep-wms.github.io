Use these configuration items to build up a cluster of Steep instances. Under
the hood, Steep uses [Vert.x](https://vertx.io/) and [Hazelcast](https://hazelcast.com/),
so these configuration items are very similar to the ones found in these two
frameworks. To build up a cluster, you need to configure an event bus connection
and a cluster connection. They should use different ports. `host` typically
refers to the machine your instance is running on and `publicHost` or
`publicAddress` specify the hostname or IP address that your Steep instance will
use in your network to advertise itself so that other instances can connect to
it.

For more information, please read the documentation of [Vert.x](https://vertx.io/)
and [Hazelcast](https://hazelcast.com/).

<dl>
<dt>`steep.cluster.eventBus.host`</dt>
<dd>
The IP address (or hostname) to bind the clustered eventbus to

**Default:** Automatically detected local network interface
</dd>

<dt>`steep.cluster.eventBus.port`</dt>
<dd>
The port the clustered eventbus should listen on

**Default:** A random port
</dd>

<dt>`steep.cluster.eventBus.publicHost`</dt>
<dd>
The IP address (or hostname) the eventbus uses to announce itself within in the cluster

**Default:** Same as `steep.cluster.eventBus.host`
</dd>

<dt>`steep.cluster.eventBus.publicPort`</dt>
<dd>
The port that the eventbus uses to announce itself within in the cluster

**Default:** Same as `steep.cluster.eventBus.port`
</dd>

<dt>`steep.cluster.hazelcast.clusterName`</dt>
<dd>
An optional cluster name that can be used to separate clusters of Steep instances. Two instances from different clusters (with different names) cannot connect to each other.

By default, no cluster name is set, which means all instances can connect to each other. However, a Steep instance without a cluster name cannot connect to a named cluster.

*Heads up:* if you have a cluster name set and you're using a <DocsLink href="#cloud-connection">cloud connection</DocsLink> to deploy remote agents on demand, make sure these Steep instances use the same cluster name. Otherwise, you won't be able to connect to them.
</dd>

<dt>`steep.cluster.hazelcast.publicAddress`</dt>
<dd>
The IP address (or hostname) and port Hazelcast uses to announce itself within in the cluster
</dd>

<dt>`steep.cluster.hazelcast.port`</dt>
<dd>
The port that Hazelcast should listen on
</dd>

<dt>`steep.cluster.hazelcast.interfaces`</dt>
<dd>
A list of IP address patterns specifying valid interfaces Hazelcast should bind to
</dd>

<dt>`steep.cluster.hazelcast.members`</dt>
<dd>
A list of IP addresses (or hostnames) of Hazelcast cluster members
</dd>

<dt>`steep.cluster.hazelcast.tcpEnabled`</dt>
<dd>
`true` if Hazelcast should use TCP to connect to other instances, `false` if it should use multicast

**Default:** `false`
</dd>

<dt>`steep.cluster.hazelcast.placementGroupName`</dt>
<dd>
An optional name specifying in which group this Hazelcast member should be placed. Steep uses [distributed maps](https://docs.hazelcast.com/hazelcast/5.1/data-structures/map) to share data between instances. Data in these maps is partitioned (i.e. distributed to the individual cluster members). In a large cluster, no member keeps all the data. Most nodes only keep a small fraction of the data (a partition).

To make sure data is not lost if a member goes down, Hazelcast uses backups to distribute copies of the data across the cluster. By specifying a placement group, you can control how Hazelcast distributes these backups. Hazelcast will always prefer creating backups in a group that does not own the data so that if all members of a group go down, the other group still has all the backup data.

Examples for sensible groups are racks, data centers, or availability zones.

For more information, see the following links:

* https://docs.hazelcast.com/hazelcast/5.1/architecture/data-partitioning
* https://docs.hazelcast.com/hazelcast/5.1/clusters/partition-group-configuration
* https://docs.hazelcast.com/hazelcast/5.1/data-structures/backing-up-maps

Note that if you configure a placement group name, all members in your cluster must also have a placement group name. Otherwise, you will receive an exception about mismatching configuration on startup.
</dd>

<dt>`steep.cluster.hazelcast.liteMember`</dt>
<dd>
`true` if this instance should be a [Hazelcast lite member](https://docs.hazelcast.com/hazelcast/5.1/maintain-cluster/lite-members). Lite members do not own any in-memory data. They are mainly used for compute-intensive tasks. With regard to Steep, an instance with a controller and a scheduler should not be a lite member, because these components heavily rely on internal state. A Steep instance that only contains an agent and therefore only executes services, however, could be a lite member. See the <DocsLink href="#software-architecture">architecture section</DocsLink> for more information about these components.

Your cluster cannot consist of only lite members. Otherwise, it is not able to maintain internal state at all.

Note that since lite members cannot keep data, they are not suitable to keep backups either. See `steep.cluster.hazelcast.placementGroupName` for more information. For reasons of reliability, a cluster should contain at least three full (i.e. non-lite) members.
</dd>

<dt>`steep.cluster.lookupOrphansInterval`</dt>
<dd>
The interval at which Steep's main thread looks for orphaned entries in its internal remote agent registry (specified as a <DocsLink href="#durations">duration</DocsLink>). Such entries may (very rarely) happen if there is a network failure during deregistration of an agent. You normally do not have to change this configuration.

**Default:** 5m
</dd>

<dt>`steep.cluster.hazelcast.restoreMembersOnStartup.enabled`</dt>
<dd>
`true` if Steep should try to load IP addresses of possibly still running VMs from its database during startup and add them to `steep.cluster.hazelcast.members`. This is useful if a Steep instance has crashed and should be reintegrated into an existing cluster when it's back.

**Default:** `false`
</dd>

<dt>`steep.cluster.hazelcast.restoreMembersOnStartup.defaultPort`</dt>
<dd>
If `steep.cluster.hazelcast.restoreMembersOnStartup.enabled` is `true`, potential Hazelcast cluster members will be restored from database. This configuration item specifies on which Hazelcast port these members are listening.
</dd>

<dt>`steep.cluster.hazelcast.splitBrainProtection.enabled`</dt>
<dd>
`true` if split-brain protection should be enabled. This mechanism makes sure the cluster is only able to operate if there are at least `n` members, where `n` is defined by `steep.cluster.hazelcast.splitBrainProtection.minClusterSize`. If there are less than `n` members, Steep instances in the cluster will not be able to access cluster-wide data structures and stop to operate until the issue has been resolved.

This mechanism protects against so-called split-brain situations where one part of the cluster loses connection to another part, and the cluster is therefore split into different partitions. If one partition becomes too small, it should stop operating to avoid doing any harm.

See the [Hazelcast documentation](https://docs.hazelcast.com/imdg/4.2/network-partitioning/split-brain-protection) for more information.

**Default:** `false`
</dd>

<dt>`steep.cluster.hazelcast.splitBrainProtection.minClusterSize`</dt>
<dd>
The minimum number of members the cluster must have to be able operate if split-brain protection is enabled.

Recommendation: Your cluster should have an odd number of members. The minimum cluster size should be even and represent the majority of your cluster. For example, if your cluster has 7 nodes, set this value to 4. This makes sure that when a split-brain situation happens, the majority of your cluster will be able to continue operating while the smaller part will stop.

This configuration item does not have a default value. It must be set if `steep.cluster.hazelcast.splitBrainProtection.enable` equals `true`.
</dd>

<dt>`steep.cluster.hazelcast.splitBrainProtection.gracefulStartup`</dt>
<dd>
`true` if the split-brain protection mechanism should only start to be in effect once the cluster has reached its minimum size. This allows the cluster to startup gracefully even if the member count is temporarily lower than the defined minimum.

**Default:** `true`
</dd>

<dt>`steep.cluster.hazelcast.splitBrainProtection.exitProcessAfter`</dt>
<dd>
An optional timeout (specified as a <DocsLink href="#durations">duration</DocsLink>) defining how long a Steep instance may keep running after a split-brain situation has been detected. When the timeout is reached and the split-brain situation has not been resolved in the meantime, the Steep instance shuts itself down with exit code 16. This mechanism can be used to prevent a Steep instance from doing any harm when it is in a split-brain situation.
</dd>

<dt>`steep.cluster.kubernetes.enabled`</dt>
<dd>
`true` if Hazelcast should try to automatically discover other members in a Kubernetes cluster

**Default:** `false`
</dd>

<dt>`steep.cluster.kubernetes.namespace`</dt>
<dd>
The Kubernetes namespace where Steep is running. Falls back to the values of the environment variables `KUBERNETES_NAMESPACE` or `OPENSHIFT_BUILD_NAMESPACE`, or to the pod's namespace retrieved from `/var/run/secrets/kubernetes.io/serviceaccount/namespace`.
</dd>

<dt>`steep.cluster.kubernetes.serviceName`</dt>
<dd>
An optional service name to limit the cluster members to pods that are connected to a given service. If not specified, all pods in the namespace will be considered potential cluster members.
</dd>

<dt>`steep.cluster.kubernetes.serviceDns`</dt>
<dd>
Instead of specifying a namespace and a service name through `steep.cluster.kubernetes.namespace` and `steep.cluster.kubernetes.serviceName`, you can also specify a DNS name in the form `SERVICE-NAME.NAMESPACE.svc.cluster.local`. Hazelcast will perform a DNS lookup to obtain the pod IP addresses of cluster members.
</dd>
</dl>

