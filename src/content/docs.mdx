import CodeExample from "../components/CodeExample"
import ScrollLink from "../components/ScrollLink"
import Layout from "./docs"
import "./docs.scss"

import Architecture from "../assets/architecture.svg"
import GenerateProcessChains from "../assets/generate-process-chains.svg?include"

export default Layout

## Documentation

In this section, we describe the individual features of Steep. The
documentation always applies to the latest software version.

###### Table of contents

<div className="table-of-contents"></div>

### How does Steep work?

In order to answer this question, we will first describe how Steep transforms
scientific workflow graphs into executable units. After that, we will have a
look at Steep's software architecture and what kind of processing services it
can execute.

(This section is based on the following publication: Kr√§mer, M. (2020).
Capability-Based Scheduling of Scientific Workflows in the Cloud.
*Proceedings of the 9th International Conference on Data Science, Technology, and Applications DATA*.)

#### Workflow scheduling

Steep is a scientific workflow management system that can be used to control
the processing of very large data sets in a distributed environment.

A scientific workflow is typically represented by a directed acyclic graph that
describes how an input data set is processed by certain tasks in a given order
to produce a desired outcome. Such workflows can become very large with
hundreds up to several thousands of tasks processing data volumes ranging from
gigabytes to terabytes. The following figure shows a simple example of such a
workflow in an extended Petri Net notation proposed by
[van der Aalst and van Hee (2004)](https://doi.org/10.7551/mitpress/7301.001.0001).

<div className="docs-image docs-image-generate-process-chains"
  dangerouslySetInnerHTML={{ __html: GenerateProcessChains }} />

In this example, an input file is first processed by a task A. This task
produces two results. The first one is processed by task B whose result is in
turn sent to C. The second result of A is processed by D. The outcomes of C and
D are finally processed by task E.

In order to be able to schedule such a workflow in a distributed environment,
the graph has to be transformed to individual executable units. Steep follows a
hybrid scheduling approach that applies heuristics on the level of the workflow
graph and later on the level of individual executable units. We assume that
tasks that access the same data should be executed on the same machine to
reduce the communication overhead and to improve file reuse. We therefore
group tasks into so-called <ScrollLink href="#process-chains">process chains</ScrollLink>,
which are linear sequential lists (without branches and loops).

Steep transforms workflows to process chains in an iterative manner. In each
iteration, it finds the longest linear sequences of tasks and groups them to
process chains. The following animation shows how this works for our example
workflow:

<div className="docs-image docs-image-generate-process-chains-animated"
  dangerouslySetInnerHTML={{ __html: GenerateProcessChains }} />

Task A will be put into a process chain in iteration 1. Steep then schedules
the execution of this process chain. After the execution has finished,
Steep uses the results to produce a process chain containing B and C and
another one containing D. These process chains are then scheduled to be
executed in parallel. The results are finally used to generate the fourth
process chain containing task E, which is also scheduled for execution.

#### Software architecture

The following figure shows the main components of Steep: the HTTP server,
the controller, the scheduler, the agent, and the cloud manager.

<img className="docs-image docs-image-architecture" src={Architecture} />

Together, these components form an instance of Steep. In practice, a single
instance typically runs on a separate virtual machine, but multiple instances
can also be started on the same machine. Each component can be enabled or
disabled in a given instance (see the <ScrollLink href="#configuration">configuration options</ScrollLink> 
for more information). That means, in a cluster, there can be instances
that have all five components enabled, and others that have only an agent,
for example.

All components of all instances communicate with each other through messages
sent over an event bus. Further, the HTTP server, the controller, and the
scheduler are able to connect to a shared database.

The HTTP server provides information about scheduled, running, and finished
workflows to clients. Clients can also upload a new workflow. In this case,
the HTTP server puts the workflow into the database and sends a message to one
of the instances of the controller.

The controller receives this message, loads the workflow from the database,
and starts transforming it iteratively to process chains as
described <ScrollLink href="#workflow-scheduling">above</ScrollLink>.
Whenever it has generated new process chains, it puts them into the database
and sends a message to all instances of the scheduler.

The schedulers then select agents to execute the process chains. They load the
process chains from the database, send them via the event bus to the selected
agents for execution, and finally write the results into the database. The
schedulers also send a message back to the controller so it can continue with
the next iteration and generate more process chains until the workflow has been
completely transformed.

In case a scheduler does not find an agent suitable for the execution of a
process chain, it sends a message to the cloud manager (a component that
interacts with the API of the Cloud infrastructure) and asks it to create a
new agent.

#### Processing services

Steep is very flexible and allows a wide range of processing services (or
microservices) to be integrated. A typical processing service is a program
that reads one or more input files and writes one or more output files. The
program may also accept generic parameters. The service can be implemented in
any programming language (as long as the binary or script is executable on the
machine the Steep agent is running) or can be wrapped in a Docker container.

For a seamless integration, a processing service should adhere to the following
guidelines:

* Every processing service should be a microservice. It should run in
  its own process and serve one specific purpose.
* As Steep needs to call the service in a distributed environment, it should
  not have a graphical user interface or require any human interaction during
  the runtime. Suitable services are command-line applications that accept
  arguments to specify input files, output files, and parameters.
* The service should read from input files, process the data, write results to
  output files, and then exit. It should not run continuously like a web
  service. If you need to integrate a web service in your workflow, we
  recommend using the `curl` command or something similar.
* Steep does not require the processing services to implement a specific
  interface. Instead, the service's input and output parameters should be
  described in a special data model called <ScrollLink href="#service-metadata">service metadata</ScrollLink>.
* According to common conventions for exit codes, a processing service should
  return 0 (zero) upon successful execution and any number but zero in case an
  error has occurred (e.g. 1, 2, 128, 255, etc.).
* In order to ensure deterministic workflow exceutions, services should be
  stateless and idempotent. This means that every execution of a service with
  the same input data and the same set of parameters should produce the same
  result.

### Example workflows

In this section, we describe example workflows covering patterns we regularly
see in real-world use cases. For each workflow, we also provide the required
service metadata. For more information about the <ScrollLink href="#workflows">workflow model</ScrollLink>
and <ScrollLink href="#service-metadata">service metadata</ScrollLink>, please read
the section on <ScrollLink href="#data-models">data models</ScrollLink>.

#### Running two services in parallel

This example workflow consists of two actions that each copy a file. Since both
actions do not depend on each other (i.e. they do not share any variable), Steep
converts them to two independent process chains and executes them in parallel
(as long as there are at least two agents available).

The workflow defines four variables. `inputFile1` and `inputFile2` point to the
two files to be copied. `outputFile1` and `outputFile2` have no value. Steep
will create unique values (output file names) for them during the workflow
execution.

The workflow then specifies two execute actions for the `copy` service. The
service metadata of `copy` defines that this processing service has an input
parameter `input_file` and an output parameter `output_file`, both of which
must be specified exactly one time (`cardinality` equals `1..1`).

For each execute action, Steep assigns the input variables to the input
parameters, generates file names for the output variables, and then executes the
processing services.

<CodeExample title="Workflow:">

```json code-example
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile1",
    "value": "example1.txt"
  }, {
    "id": "outputFile1"
  }, {
    "id": "inputFile2",
    "value": "example2.txt"
  }, {
    "id": "outputFile2"
  }],
  "actions": [{
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile1"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile2"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile2"
    }]
  }]
}
```

</CodeExample>

<CodeExample title="Service metadata:">

```json code-example
[{
  "id": "copy",
  "name": "Copy",
  "description": "Copy files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}]
```

</CodeExample>

#### Chaining two services

The following example workflow makes a copy of a file and then a copy of the
copy (i.e. the file is copied and the result is copied again). The workflow
contains two actions that share the same variable: `outputFile1` is used as
the output of the first action and as the input of the second action. Steep
executes them in sequence.

The service metadata for this workflow is the same as for the previous one.

<CodeExample title="Workflow:">

```json code-example
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile",
    "value": "example.txt"
  }, {
    "id": "outputFile1"
  }, {
    "id": "outputFile2"
  }],
  "actions": [{
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile1"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "outputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile2"
    }]
  }]
}
```

</CodeExample>

#### Splitting and joining results

This example starts with an action that copies a file. Two other actions then
run in parallel and make copies of the result of the first action. A final
action then joins these copies to a single file. The workflow has a
split-and-join pattern because the graph is split into two branches after the
first action. These branches are then joined into a single one with the final
action.

<CodeExample title="Workflow:">

```json code-example
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile",
    "value": "example.txt"
  }, {
    "id": "outputFile1"
  }, {
    "id": "outputFile2"
  }, {
    "id": "outputFile3"
  }, {
    "id": "outputFile4"
  }],
  "actions": [{
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile1"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "outputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile2"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "outputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile3"
    }]
  }, {
    "type": "execute",
    "service": "join",
    "inputs": [{
      "id": "i",
      "var": "outputFile2"
    }, {
      "id": "i",
      "var": "outputFile3"
    }],
    "outputs": [{
      "id": "o",
      "var": "outputFile4"
    }]
  }]
}
```

</CodeExample>

<CodeExample title="Service metadata:">

```json code-example
[{
  "id": "copy",
  "name": "Copy",
  "description": "Copy files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}, {
  "id": "join",
  "name": "Join",
  "description": "Merge one or more files into one",
  "path": "join.sh",
  "runtime": "other",
  "parameters": [{
    "id": "i",
    "name": "Input files",
    "description": "One or more input files to merge",
    "type": "input",
    "cardinality": "1..n",
    "data_type": "file"
  }, {
    "id": "o",
    "name": "Output file",
    "description": "The output file",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}]
```

</CodeExample>

#### Processing a dynamic number of results in parallel

This example demonstrates how to process the results of an action in parallel
even if the number of result files is unknown during the design of the workflow.
The workflow starts with an action that splits an input file `inputFile` into
multiple files (e.g. one file per line) stored in a directory `outputDirectory`.
A for-each action then iterates over these files and creates copies. The
for-each action has an iterator `i` that serves as the input for the individual
instances of the `copy` service. The output files (`outputFile1`) of this service
are collected via the `yieldToOutput` property in a variable called `copies`.
The final `join` service merges these copies into a single file `outputFile2`.

<CodeExample title="Workflow:">

```json code-example
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile",
    "value": "example.txt"
  }, {
    "id": "lines",
    "value": 1
  }, {
    "id": "outputDirectory"
  }, {
    "id": "i"
  }, {
    "id": "outputFile1"
  }, {
    "id": "copies"
  }, {
    "id": "outputFile2"
  }],
  "actions": [{
    "type": "execute",
    "service": "split",
    "parameters": [{
      "id": "lines",
      "var": "lines"
    }],
    "inputs": [{
      "id": "file",
      "var": "inputFile"
    }],
    "outputs": [{
      "id": "output_directory",
      "var": "outputDirectory"
    }]
  }, {
    "type": "for",
    "input": "outputDirectory",
    "enumerator": "i",
    "output": "copies",
    "actions": [{
      "type": "execute",
      "service": "copy",
      "inputs": [{
        "id": "input_file",
        "var": "i"
      }],
      "outputs": [{
        "id": "output_file",
        "var": "outputFile1"
      }]
    }],
    "yieldToOutput": "outputFile1"
  }, {
    "type": "execute",
    "service": "join",
    "inputs": [{
      "id": "i",
      "var": "copies"
    }],
    "outputs": [{
      "id": "o",
      "var": "outputFile2"
    }]
  }]
}
```

</CodeExample>

<CodeExample title="Service metadata:">

```json code-example
[{
  "id": "split",
  "name": "Split",
  "description": "Split a file into pieces",
  "path": "split",
  "runtime": "other",
  "parameters": [{
    "id": "lines",
    "name": "Number of lines per file",
    "description": "Create smaller files n lines in length",
    "type": "argument",
    "cardinality": "0..1",
    "data_type": "integer",
    "label": "-l"
  }, {
    "id": "file",
    "name": "Input file",
    "description": "The input file to split",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_directory",
    "name": "Output directory",
    "description": "The output directory",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "directory",
    "file_suffix": "/"
  }]
}, {
  "id": "copy",
  "name": "Copy",
  "description": "Copy files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}, {
  "id": "join",
  "name": "Join",
  "description": "Merge one or more files into one",
  "path": "join.sh",
  "runtime": "other",
  "parameters": [{
    "id": "i",
    "name": "Input files",
    "description": "One or more input files to merge",
    "type": "input",
    "cardinality": "1..n",
    "data_type": "file"
  }, {
    "id": "o",
    "name": "Output file",
    "description": "The output file",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}]
```

</CodeExample>

#### Feeding results back into the workflow (cycles/loops)

TODO

### Data models

#### Workflows

TODO

#### Process chains

TODO

#### Submissions

A *submission* is created when you submit a <ScrollLink href="#workflows">workflow</ScrollLink>
through the <ScrollLink href="#get-submissions">`/workflows`</ScrollLink> endpoint. It contains information
about the workflow execution such as the start and end time as well as the
current <ScrollLink href="#submission-status">status</ScrollLink>.

| Property               | Type   | Description
| ---------------------- | ------ | -----------
| id                     | string | Unique submission identifier
| workflow               | object | The submitted <ScrollLink href="#workflows">workflow</ScrollLink>
| startTime              | string | An ISO 8601 timestamp denoting the date and time when the workflow execution was started. May be `null` if the execution has not started yet.
| endTime                | string | An ISO 8601 timestamp denoting the date and time when the workflow execution finished. May be `null` if the execution has not finished yet.
| status                 | string | The current <ScrollLink href="#submission-status">status</ScrollLink> of the submission
| runningProcessChains   | number | The number of <ScrollLink href="#process-chains">process chains</ScrollLink> currently being executed
| cancelledProcessChains | number | The number of process chains that have been cancelled
| succeededProcessChains | number | The number of process chains that have finished successfully
| failedProcessChains    | number | The number of process chains whose execution has failed
| totalProcessChains     | number | The current total number of process chains in this submission. May increase during execution when new process chains are generated.
| results                | object | If `status` is `SUCCESS` or `PARTIAL_SUCCESS`, this property contains the list of workflow result files grouped by their output variable ID. Otherwise, it is `null`.
| errorMessage           | string | If `status` is `ERROR`, this property contains a human-readable error message. Otherwise, it is `null`.

<CodeExample>

```json code-example
{
  "id": "aiq7eios7ubxglkcqx5a",
  "workflow": {
    "api": "4.0.0",
    "vars": [{
      "id": "myInputFile",
      "value": "/data/input.txt"
    }, {
      "id": "myOutputFile"
    }],
    "actions": [{
      "type": "execute",
      "service": "cp",
      "inputs": [{
        "id": "input_file",
        "var": "myInputFile"
      }],
      "outputs": [{
        "id": "output_file",
        "var": "myOutputFile",
        "store": true
      }],
      "parameters": []
    }]
  },
  "startTime": "2020-02-13T15:38:58.719382Z",
  "endTime": "2020-02-13T15:39:00.807715Z",
  "status": "SUCCESS",
  "runningProcessChains": 0,
  "cancelledProcessChains": 0,
  "succeededProcessChains": 1,
  "failedProcessChains": 0,
  "totalProcessChains": 1,
  "results": {
    "myOutputFile": [
      "/data/out/aiq7eios7ubxglkcqx5a/aiq7hygs7ubxglkcrf5a"
    ]
  }
}
```

</CodeExample>

#### Submission status

The following table shows the statuses a <ScrollLink href="#submissions">submission</ScrollLink>
can have:

| Status          | Description
| --------------- | -----------
| ACCEPTED        | The submission has been accepted by Steep but execution has not started yet
| RUNNING         | The submission is currently being executed
| CANCELLED       | The submission was cancelled
| SUCCESS         | The execution of the submission finished successfully
| PARTIAL_SUCCESS | The submission was executed completely but one or more process chains failed
| ERROR           | The execution of the submission failed

#### Process chain status

The following table shows the statuses a <ScrollLink href="#process-chains">process chain</ScrollLink>
can have:

| Status          | Description
| --------------- | -----------
| REGISTERED      | The process chain has been created but execution has not started yet
| RUNNING         | The process chain is currently being executed
| CANCELLED       | The execution of the process chain was cancelled
| SUCCESS         | The process chain was executed successfully
| ERROR           | The execution of the process chain failed

#### Service metadata

Service metadata is used to describe the interface of a processing service so
it can be executed by Steep.

| Property                              | Type   | Description
| ------------------------------------- | ------ | -----------
| id<br/>*(required)*                    | string | A unique service identifier
| name<br/>*(required)*                  | string | A human-readable name
| description<br/>*(required)*           | string | A human-readable description
| path<br/>*(required)*                  | string | Relative path to the service executable in the service artefact (or a Docker image if `runtime` equals `docker`)
| runtime<br/>*(required)*               | string | The <ScrollLink href="#runtime-environments">runtime environment</ScrollLink>
| parameters<br/>*(required)*            | array  | A list of <ScrollLink href="#service-parameters">service parameters</ScrollLink>
| runtime_args<br/>*(optional)*          | array  | An optional list of <ScrollLink href="#runtime-arguments">arguments</ScrollLink> to pass to the runtime
| required_capabilities<br/>*(optional)* | array  | A set of strings specifying capabilities a host system must provide to be able to execute this service. See also <ScrollLink href="#setups">setups</ScrollLink>.

<CodeExample>

```json code-example
{
  "id": "cp",
  "name": "cp",
  "description": "Copies files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "no_overwrite",
    "name": "No overwrite",
    "description": "Do not overwrite existing file",
    "type": "argument",
    "cardinality": "1..1",
    "label": "-n",
    "data_type": "boolean",
    "default": false
  }, {
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}
```

</CodeExample>

#### Runtime environments

Steep provides a set of default runtime environments that define how
processing services are executed. More environments can be added through <ScrollLink href="#custom-runtime-environments">runtime environment plugins</ScrollLink>.

| Name    | Description
| ------- | -----------
| docker  | The service will be executed through Docker. The service metadata attribute `path` specifies the Docker image to run. The attribute `runtime_args` specifies parameters that should be forwarded to the `docker` `run` command.
| other   | The service will be executed like a normal executable program (binary or shell script)

#### Service parameters

TODO

#### Runtime arguments

TODO

#### Setups

TODO

### HTTP endpoints

#### Get submissions

### Configuration

TODO

#### Overview of configuration files

TODO

### Extending Steep through plugins

TODO

#### Custom runtime environments

TODO

#### Output adapters

TODO

#### Process chain adapters

TODO
