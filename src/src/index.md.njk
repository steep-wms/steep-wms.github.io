---
layout: index.njk
---

## Download and get started

In order to use Steep, you need to download it first. Choose from one of the
following options:

<div class="download-btns">
<a href="https://github.com/steep-wms/steep/releases/download/v5.0.1/steep-5.0.1.zip" class="btn btn-primary"><i data-feather="download"></i> Download Steep 5.0.1 (binaries)</a>
<a href="https://hub.docker.com/r/steep/steep/" class="btn btn-secondary">{% simpleIcon "docker" %} Docker image</a>
<a href="https://github.com/steep-wms/steep" class="btn btn-secondary"><i data-feather="github"></i> Source code</a>
</div>

If you downloaded the binary package of Steep, extract the ZIP file and run the
start script:

```bash
cd steep-5.0.1
bin/steep
```

Or, start the [Docker image](https://hub.docker.com/orgs/steep/steep) as
follows:

```bash
docker run --name steep -d --rm -p 8080:8080 \
    -e STEEP_HTTP_HOST=0.0.0.0 steep/steep:unstable
```

After a few seconds, you should be able to access Steep's web interface on
<http://localhost:8080/>.

We will now submit a simple workflow to test if Steep is running correctly. The
workflow consists of a single `sleep` command that waits for 10 seconds and
then quits. Execute the following command:

```bash
curl -X POST http://localhost:8080/workflows -d 'api: 4.0.0
vars:
  - id: sleep_seconds
    value: 10
actions:
  - type: execute
    service: sleep
    parameters:
      - id: seconds
        var: sleep_seconds'
```

The command will return the ID of the submitted workflow. You can monitor the
execution in the web interface or by issuing the following command:

```bash
curl http://localhost:8080/workflows/<workflow-id>
```

Replace `<workflow-id>` with the returned ID.

*Congratulations! You successfully installed Steep and ran your first workflow.*

## Documentation

In this section, we describe the individual features of Steep. The
documentation always applies to the latest version.

**Table of contents**

{{ docsTocPlaceholder }}

### How does Steep work?

TODO

### Example workflows

In this section, we describe example workflows covering patterns we regularly
see in real-world use cases. For each workflow, we also provide the required
service metadata. For more information about the [workflow model](#workflows)
and [service metadata](#servie-metadata), please read the section on
[data models](#data-models).

#### Running two services in parallel

This example workflow consists of two actions that each copy a file. Since both
actions do not depend on each other (i.e. they do not share any variable), Steep
converts them to two independent process chains and executes them in parallel
(as long as there are at least two agents available).

The workflow defines four variables `inputFile1` and `inputFile1` point to the
two files to be copied. `outputFile1` and `outputFile2` have no value. Steep
will create unique values (output file names) for them during the workflow
execution.

The workflow then specifies two execute actions for the `copy` service. The
service metadata of `copy` defines that this processing service has an input
parameter `input_file` and an output parameter `output_file` both of which
must be specified exactly one time (`cardinality` equals `1..1`).

For each execute action, Steep assigns the input variables to the input
parameters, generates file names for the output variables, and then executes the
processing services.

<p class="h5 mt-4 mb-4">Workflow:</p>

{% codeExample "example-1-workflow" %}
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile1",
    "value": "example1.txt"
  }, {
    "id": "outputFile1"
  }, {
    "id": "inputFile2",
    "value": "example2.txt"
  }, {
    "id": "outputFile2"
  }],
  "actions": [{
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile1"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile2"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile2"
    }]
  }]
}
{% endCodeExample %}

<p class="h5 mt-4 mb-4">Service metadata:</p>

{% codeExample "example-1-service-metadata" %}
[{
  "id": "copy",
  "name": "Copy",
  "description": "Copy files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}]
{% endCodeExample %}

#### Chaining two services

The following example workflow makes a copy of a file and then a copy of the
copy (i.e. the file is copied and the result is copied again). The workflow
contains two actions that share the same variable: `outputFile1` is used as
the output of the first action and as the input of the second action. Steep
executes them in sequence.

The service metadata for this workflow is the same as for the previous one.

<p class="h5 mt-4 mb-4">Workflow:</p>

{% codeExample "example-2-workflow" %}
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile",
    "value": "example.txt"
  }, {
    "id": "outputFile1"
  }, {
    "id": "outputFile2"
  }],
  "actions": [{
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile1"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "outputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile2"
    }]
  }]
}
{% endCodeExample %}

#### Splitting and joining results

This example starts with an action that copies a file. Two other actions then
run in parallel and make copies of the result of the first action. A final
action then joins these copies to a single file. The workflow has a
split-and-join pattern because the graph is split into two branches after the
first action. These branches are then joined into a single one with the final
action.

<p class="h5 mt-4 mb-4">Workflow:</p>

{% codeExample "example-3-workflow" %}
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile",
    "value": "example.txt"
  }, {
    "id": "outputFile1"
  }, {
    "id": "outputFile2"
  }, {
    "id": "outputFile3"
  }, {
    "id": "outputFile4"
  }],
  "actions": [{
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "inputFile"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile1"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "outputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile2"
    }]
  }, {
    "type": "execute",
    "service": "copy",
    "inputs": [{
      "id": "input_file",
      "var": "outputFile1"
    }],
    "outputs": [{
      "id": "output_file",
      "var": "outputFile3"
    }]
  }, {
    "type": "execute",
    "service": "join",
    "inputs": [{
      "id": "i",
      "var": "outputFile2"
    }, {
      "id": "i",
      "var": "outputFile3"
    }],
    "outputs": [{
      "id": "o",
      "var": "outputFile4"
    }]
  }]
}
{% endCodeExample %}

<p class="h5 mt-4 mb-4">Service metadata:</p>

{% codeExample "example-3-service-metadata" %}
[{
  "id": "copy",
  "name": "Copy",
  "description": "Copy files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}, {
  "id": "join",
  "name": "Join",
  "description": "Merge one or more files into one",
  "path": "join.sh",
  "runtime": "other",
  "parameters": [{
    "id": "i",
    "name": "Input files",
    "description": "One or more input files to merge",
    "type": "input",
    "cardinality": "1..n",
    "data_type": "file"
  }, {
    "id": "o",
    "name": "Output file",
    "description": "The output file",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}]
{% endCodeExample %}

#### Dynamically processing results in parallel

This example demonstrates how to process the results of an action in parallel
even if the number of result files is unknown during the design of the workflow.
The workflow starts with an action that splits an input file `inputFile` into
multiple files (e.g. one file per line) stored in a directory `outputDirectory`.
A for-each action then iterates over these files and creates copies. The
for-each action has an iterator `i` that serves as the input for the individual
instances of the `copy` service. The output files (`outputFile1`) of this service
are collected via the `yieldToOutput` property in a variable called `copies`.
The final `join` service merges these copies into a single file `outputFile2`.

<p class="h5 mt-4 mb-4">Workflow:</p>

{% codeExample "example-4-workflow" %}
{
  "api": "4.0.0",
  "vars": [{
    "id": "inputFile",
    "value": "example.txt"
  }, {
    "id": "lines",
    "value": 1
  }, {
    "id": "outputDirectory"
  }, {
    "id": "i"
  }, {
    "id": "outputFile1"
  }, {
    "id": "copies"
  }, {
    "id": "outputFile2"
  }],
  "actions": [{
    "type": "execute",
    "service": "split",
    "parameters": [{
      "id": "lines",
      "var": "lines"
    }],
    "inputs": [{
      "id": "file",
      "var": "inputFile"
    }],
    "outputs": [{
      "id": "output_directory",
      "var": "outputDirectory"
    }]
  }, {
    "type": "for",
    "input": "outputDirectory",
    "enumerator": "i",
    "output": "copies",
    "actions": [{
      "type": "execute",
      "service": "copy",
      "inputs": [{
        "id": "input_file",
        "var": "i"
      }],
      "outputs": [{
        "id": "output_file",
        "var": "outputFile1"
      }]
    }],
    "yieldToOutput": "outputFile1"
  }, {
    "type": "execute",
    "service": "join",
    "inputs": [{
      "id": "i",
      "var": "copies"
    }],
    "outputs": [{
      "id": "o",
      "var": "outputFile2"
    }]
  }]
}
{% endCodeExample %}

<p class="h5 mt-4 mb-4">Service metadata:</p>

{% codeExample "example-4-service-metadata" %}
[{
  "id": "split",
  "name": "Split",
  "description": "Split a file into pieces",
  "path": "split",
  "runtime": "other",
  "parameters": [{
    "id": "lines",
    "name": "Number of lines per file",
    "description": "Create smaller files n lines in length",
    "type": "argument",
    "cardinality": "0..1",
    "data_type": "integer",
    "label": "-l"
  }, {
    "id": "file",
    "name": "Input file",
    "description": "The input file to split",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_directory",
    "name": "Output directory",
    "description": "The output directory",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "directory",
    "file_suffix": "/"
  }]
}, {
  "id": "copy",
  "name": "Copy",
  "description": "Copy files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}, {
  "id": "join",
  "name": "Join",
  "description": "Merge one or more files into one",
  "path": "join.sh",
  "runtime": "other",
  "parameters": [{
    "id": "i",
    "name": "Input files",
    "description": "One or more input files to merge",
    "type": "input",
    "cardinality": "1..n",
    "data_type": "file"
  }, {
    "id": "o",
    "name": "Output file",
    "description": "The output file",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}]
{% endCodeExample %}

#### Feeding results back into the workflow (cycles/loops)

TODO

<section id="sec-data-models">

### Data models

#### Workflows

TODO

#### Process chains

TODO

#### Submissions

A *submission* is created when you submit a [workflow](#workflows) through the
[`/workflows`](#get-submissions) endpoint. It contains information about the
workflow execution such as the start and end time as well as the current
[status](#submission-status).

| Property               | Type   | Description
| ---------------------- | ------ | -----------
| id                     | string | Unique submission identifier
| workflow               | object | The submitted [workflow](#workflows)
| startTime              | string | An ISO 8601 timestamp denoting the date and time when the workflow execution was started. May be `null` if the execution has not started yet.
| endTime                | string | An ISO 8601 timestamp denoting the date and time when the workflow execution finished. May be `null` if the execution has not finished yet.
| status                 | string | The current [status](#submission-status) of the submission
| runningProcessChains   | number | The number of [process chains](#process-chains) currently being executed
| cancelledProcessChains | number | The number of process chains that have been cancelled
| succeededProcessChains | number | The number of process chains that have finished successfully
| failedProcessChains    | number | The number of process chains whose execution has failed
| totalProcessChains     | number | The current total number of process chains in this submission. May increase during execution when new process chains are generated.
| results                | object | If `status` is `SUCCESS` or `PARTIAL_SUCCESS`, this property contains the list of workflow result files grouped by their output variable ID. Otherwise, it is `null`.
| errorMessage           | string | If `status` is `ERROR`, this property contains a human-readable error message. Otherwise, it is `null`.

{% codeExample "submission" %}
{
  "id": "aiq7eios7ubxglkcqx5a",
  "workflow": {
    "api": "4.0.0",
    "vars": [{
      "id": "myInputFile",
      "value": "/data/input.txt"
    }, {
      "id": "myOutputFile"
    }],
    "actions": [{
      "type": "execute",
      "service": "cp",
      "inputs": [{
        "id": "input_file",
        "var": "myInputFile"
      }],
      "outputs": [{
        "id": "output_file",
        "var": "myOutputFile",
        "store": true
      }],
      "parameters": []
    }]
  },
  "startTime": "2020-02-13T15:38:58.719382Z",
  "endTime": "2020-02-13T15:39:00.807715Z",
  "status": "SUCCESS",
  "runningProcessChains": 0,
  "cancelledProcessChains": 0,
  "succeededProcessChains": 1,
  "failedProcessChains": 0,
  "totalProcessChains": 1,
  "results": {
    "myOutputFile": [
      "/data/out/aiq7eios7ubxglkcqx5a/aiq7hygs7ubxglkcrf5a"
    ]
  }
}
{% endCodeExample %}

#### Submission status

The following table shows the statuses a [submission](#submissions) can have:

| Status          | Description
| --------------- | -----------
| ACCEPTED        | The submission has been accepted by Steep but execution has not started yet
| RUNNING         | The submission is currently being executed
| CANCELLED       | The submission was cancelled
| SUCCESS         | The execution of the submission finished successfully
| PARTIAL_SUCCESS | The submission was executed completely but one or more process chains failed
| ERROR           | The execution of the submission failed

#### Process chain status

The following table shows the statuses a [process chain](#process-chains) can have:

| Status          | Description
| --------------- | -----------
| REGISTERED      | The process chain has been created but execution has not started yet
| RUNNING         | The process chain is currently being executed
| CANCELLED       | The execution of the process chain was cancelled
| SUCCESS         | The process chain was executed successfully
| ERROR           | The execution of the process chain failed

#### Service metadata

Service metadata is used to describe the interface of a processing service so
it can be executed by Steep.

| Property                              | Type   | Description
| ------------------------------------- | ------ | -----------
| id<br>*(required)*                    | string | A unique service identifier
| name<br>*(required)*                  | string | A human-readable name
| description<br>*(required)*           | string | A human-readable description
| path<br>*(required)*                  | string | Relative path to the service executable in the service artefact (or a Docker image if `runtime` equals `docker`)
| runtime<br>*(required)*               | string | The [runtime environment](#runtime-environments)
| parameters<br>*(required)*            | array  | A list of [service parameters](#service-parameters)
| runtime_args<br>*(optional)*          | array  | An optional list of [arguments](#runtime-arguments) to pass to the runtime
| required_capabilities<br>*(optional)* | array  | A set of strings specifying capabilities a host system must provide to be able to execute this service. See also [setups](#setups).

{% codeExample "service-metadata" %}
{
  "id": "cp",
  "name": "cp",
  "description": "Copies files",
  "path": "cp",
  "runtime": "other",
  "parameters": [{
    "id": "no_overwrite",
    "name": "No overwrite",
    "description": "Do not overwrite existing file",
    "type": "argument",
    "cardinality": "1..1",
    "label": "-n",
    "data_type": "boolean",
    "default": false
  }, {
    "id": "input_file",
    "name": "Input file name",
    "description": "Input file name",
    "type": "input",
    "cardinality": "1..1",
    "data_type": "file"
  }, {
    "id": "output_file",
    "name": "Output file name",
    "description": "Output file name",
    "type": "output",
    "cardinality": "1..1",
    "data_type": "file"
  }]
}
{% endCodeExample %}

#### Runtime environments

Steep provides a set of default runtime environments that define how
processing services are executed. More environments can be added through
[runtime environment plugins](#custom-runtime-environments).

| Name    | Description
| ------- | -----------
| docker  | The service will be executed through Docker. The service metadata attribute `path` specifies the Docker image to run. The attribute `runtime_args` specifies parameters that should be forwarded to the `docker` `run` command.
| other   | The service will be executed like a normal executable program (binary or shell script)

#### Service parameters

TODO

#### Runtime arguments

TODO

#### Setups

TODO

</section>

### HTTP endpoints

#### Get submissions

### Configuration

TODO

### Extending Steep through plugins

TODO

#### Custom runtime environments

TODO

#### Output adapters

TODO

#### Process chain adapters

TODO

## About

Steep's development is led by the competence center for
[Spatial Information Management](https://igd.fraunhofer.de/en/competences/technologies/spatial-information-management)
of the [Fraunhofer Institute for Computer Graphics Research IGD](https://igd.fraunhofer.de/)
in Darmstadt, Germany. *Fraunhofer IGD* is the international leading research
institution for applied visual computing. The competence center for *Spatial
Information Management* offers expertise and innovative technologies that
enable successful communication and efficient cooperation with the help of
geographic information.

*Steep* was ini­tially de­veloped within the re­search pro­ject "IQmu­lus" (A
High-volume Fu­sion and Ana­lysis Plat­form for Geo­spa­tial Point Clouds, Cov­er­ages
and Volu­met­ric Data Sets) fun­ded from the 7<sup>th</sup> Frame­work Pro­gramme of the
European Com­mis­sion, call iden­ti­fier FP7-ICT-2011-8, un­der the Grant agree­ment
no. 318787 from 2012 to 2016. It was pre­vi­ously called the 'IQmu­lus Job­Man­ager'
or just the 'Job­Man­ager'.

#### Publications

Steep and its predecessor JobManager have ap­peared in at least the fol­low­ing
pub­lic­a­tions:

<div class="research">
    <div class="research-item">
        <div class="research-image">
            <a href="https://michelkraemer.com/publications/2018/Kraemer,%20M.%20(2018).%20A%20Microservice%20Architecture%20for%20the%20Processing%20of%20Large%20Geospatial%20Data%20in%20the%20Cloud.pdf"><img src="https://michelkraemer.com/images/research/18-01.jpg" class="img-fluid"></a>
        </div>
        <div class="research-body">
            <div class="csl-bib-body">
                <div class="csl-entry">Krämer, M. (2018). <i>A Mi­croservice Ar­chi­tec­ture for the Pro­cessing of Large Geo­spa­tial Data in the Cloud</i> (Doc­toral dis­ser­ta­tion). Tech­nis­che Uni­versität Darm­stadt. <a href="https://doi.org/10.13140/RG.2.2.30034.66248">ht­tps://​doi.org/​10.13140/​RG.2.2.30034.66248</a><br>
                <span class="bib-links">[ <a href="https://michelkraemer.com/publications/2018/Kraemer,%20M.%20(2018).%20A%20Microservice%20Architecture%20for%20the%20Processing%20of%20Large%20Geospatial%20Data%20in%20the%20Cloud.pdf">PDF</a> ]</span></div>
            </div>
        </div>
    </div>
    <div class="research-item">
        <div class="research-image">
            <img src="https://michelkraemer.com/images/research/16-01.jpg" class="img-fluid">
        </div>
        <div class="research-body">
            <div class="csl-bib-body">
                <div class="csl-entry">Böhm, J., Bredif, M., Gi­er­linger, T., Krämer, M., Linden­bergh, R., Liu, K., … Sir­ma­cek, B. (2016). The IQmu­lus Urban Show­case: Auto­matic Tree Clas­si­fic­a­tion and Iden­ti­fic­a­tion in Huge Mo­bile Map­ping Point Clouds. <i>IS­PRS - In­ter­na­tional Archives of the Pho­to­gram­metry, Re­mote Sens­ing and Spa­tial In­form­a­tion Sci­ences</i>, <i>XLI</i>-<i>B3</i>, 301–307. <a href="https://doi.org/10.5194/isprs-archives-XLI-B3-301-2016">ht­tps://​doi.org/​10.5194/​is­prs-archives-XLI-B3-301-2016</a></div>
            </div>
        </div>
    </div>
    <div class="research-item">
        <div class="research-image">
            <img src="https://michelkraemer.com/images/research/15-04.jpg" class="img-fluid">
        </div>
        <div class="research-body">
            <div class="csl-bib-body">
                <div class="csl-entry">Krämer, M., &amp; Sen­ner, I. (2015). A mod­u­lar soft­ware ar­chi­tec­ture for pro­cessing of big geo­spa­tial data in the cloud. <i>Com­puters &amp; Graph­ics</i>, <i>49</i>, 69–81. <a href="https://doi.org/10.1016/j.cag.2015.02.005">ht­tps://​doi.org/​10.1016/​j.cag.2015.02.005</a></div>
            </div>
        </div>
    </div>
</div>
