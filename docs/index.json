[{"slug":"what-to-read-next","body":"After youâ€™ve installed Steep, you may familiarize yourself with the concepts, go through some of our tutorials, or read the API reference.\nLearn about Steepâ€™s software architecture and how it schedules workflows.\nRead our tutorial on writing and submitting your first workflow.\nThe API reference of Steepâ€™s main data model for workflows."},{"slug":"","body":"Introduction\nChoose from the following options to download Steep:\nDownload Steep 6.8.0 (binaries)\nDocker image\nSource code\nIf youâ€™ve downloaded the binary package of Steep, extract the ZIP file and run the start script:\nTerminal\nOr, start the Docker image as follows:\nTerminal\nAfter a few seconds, you can access Steepâ€™s web interface on http://localhost:8080/.\nWe will now submit a simple workflow to test if Steep is running correctly. The workflow consists of a single execute action that sleeps for 10 seconds and then quits. Run the following command:\nTerminal\nThe command will return the ID of the submitted workflow. You can monitor the execution in the web interface or by issuing the following command:\nTerminal\nReplace [WORKFLOW ID] with the returned ID.\nCongratulations! Youâ€™ve successfully installed Steep and ran your first workflow. ðŸŽ‰\nHow does Steep work?\n"},{"slug":"what-we-are-going-to-do","body":"In this tutorial, we will apply a tiling service and a segmentation service to an aerial image. The segmentation service is based on segment-geospatial, which in turn uses Facebookâ€™s Segment Anything AI model. Both services are provided as pre-built Docker images on GitHub.\nSemantic segmentation is the process of assigning an object class (e.g. tree, building, or street) to each pixel of an image. Our workflow creates segmentation masks, which are images where the detected classes are represented by different colors.\nThe workflow consists of two steps:\nSplit input image into tiles\nThe tiling service splits the input image into four smaller tiles.\nApply image segmentation\nThe segmentation service is applied in a for-each action to each image tile to create the segmentation masks.\nThe actions in the second workflow step are intended to run in parallel. If youâ€™ve just installed Steep, you will most likely want to enable parallelization.\nNote that each instance of the segmentation service requires up 6 GB of RAM. If you wish to run the workflow on a single machine only, limit the number of agents so that you donâ€™t exceed your main memory.\nYou might wonder why the input image needs to be tiled and why we donâ€™t apply the segmentation to it directly. There are three reasons for that:\n 1. Quality. Aerial images typically have a high resolution (e.g. 5000x5000 pixels, like the one below), and AI models tend to perform badly on large images. Consider the following pictures where we applied the segmentation on an input image directly and then executed the workflow with the same image but with 2Ã—2 tiles and 5Ã—5 tiles. The smaller the tiles, the more precise the results are and the more objects the model is able to identify.\nNo tiling\n2Ã—2 tiles\n5Ã—5 tiles\n 2. Lower memory consumption. The smaller the tiles are, the less memory each segmentation process needs. This is particularly important if you enable parallelization and run multiple processes on the same machine. Also, if you enable GPU acceleration, depending on your hardware, your graphics card memory might not be large enough to process a high-resolution image and the segmentation service will crash.\n 3. For the sake of demonstration. With this tutorial, we aim to teach you how to use a for-each action to process the results of a service with another service in parallel.\nImage tiling will result in a higher quality, but it also has an impact on performance. The more tiles you create, the longer the workflow will take."},{"slug":"why-tile-the-image","body":"You might wonder why the input image needs to be tiled and why we donâ€™t apply the segmentation to it directly. There are three reasons for that:\n 1. Quality. Aerial images typically have a high resolution (e.g. 5000x5000 pixels, like the one below), and AI models tend to perform badly on large images. Consider the following pictures where we applied the segmentation on an input image directly and then executed the workflow with the same image but with 2Ã—2 tiles and 5Ã—5 tiles. The smaller the tiles, the more precise the results are and the more objects the model is able to identify.\nNo tiling\n2Ã—2 tiles\n5Ã—5 tiles\n 2. Lower memory consumption. The smaller the tiles are, the less memory each segmentation process needs. This is particularly important if you enable parallelization and run multiple processes on the same machine. Also, if you enable GPU acceleration, depending on your hardware, your graphics card memory might not be large enough to process a high-resolution image and the segmentation service will crash.\n 3. For the sake of demonstration. With this tutorial, we aim to teach you how to use a for-each action to process the results of a service with another service in parallel.\nImage tiling will result in a higher quality, but it also has an impact on performance. The more tiles you create, the longer the workflow will take."},{"slug":"download-input-data","body":"The most important thing for any data processing workflow is the data. In this tutorial, we use an aerial image provided by the Hessische Verwaltung fÃ¼r Bodenmanagement und Geoinformation in Germany through https://gds.hessen.de. The image is free and can be used for any purpose.\nUse the following link to download the full resolution image:\nhttps://raw.githubusercontent.com/steep-wms/aerial-image-segmentation/1.0.0/data/dop20_32_475_5524_1_he.jpg\nIn the following, we assume that the image has been saved to:\n/data/dop20_32_475_5524_1_he.jpg\nIf youâ€™ve downloaded the image to another location on your computer, make sure you modify the paths in the subsequent steps."},{"slug":"add-segmentation-metadata","body":"Similar to the previous tutorials, we need to add metadata for the tiling service and the segmentation service. Open the file conf/services/services.yaml and add the following code to it to describe the tiling service:\nconf/services/services.yaml\nThe service has three parameters: one for the input image file, one for the output directory where the tiles will be stored, and one specifying the number of columns and rows to split the image into. Regarding the latter, a value of 5 means the image will be split into 5Ã—5 (=25) tiles.\nAs the service is provided as a pre-compiled Docker image on GitHub, we set the attribute path to the Docker image name and the attribute runtime to docker.\nNow, add the metadata for the segmentation service:\nconf/services/services.yaml\nThe service has just two parameters: one for the image to segment and one for the filename of the output segmentation mask. We set the fileSuffix of the output parameter to .jpg to make sure the filename will be generated with the right extension.\nAgain, we use the Docker image name as path and set the runtime to docker."},{"slug":"configure-steep","body":"Our two services will be executed in Docker containers, which have a virtualized file system. Any data location on the host system that should be accessible from within a Docker container needs to be mounted when the container is started.\nIn our case, this applies to the /data directory where the input image has been downloaded. We will use the same directory for Steepâ€™s temporary path as well as its output path to access the segmentation masks after the workflow has finished.\nOpen Steepâ€™s main configuration file conf/steep.yaml and modify the properties tmpPath and outPath as follows:\nconf/steep.yaml\nIn the same file, add the following configuration to tell the Docker runtime to mount the /data directory into every container started:\nconf/steep.yaml\nRestart Steep if it is running, so it can pick up the updated configuration."},{"slug":"change-temporary-path-and-output-path","body":"Open Steepâ€™s main configuration file conf/steep.yaml and modify the properties tmpPath and outPath as follows:\nconf/steep.yaml\n"},{"slug":"mount-data-directory","body":"In the same file, add the following configuration to tell the Docker runtime to mount the /data directory into every container started:\nconf/steep.yaml\n"},{"slug":"restart-steep-segment","body":"Restart Steep if it is running, so it can pick up the updated configuration."},{"slug":"create-segmentation-workflow","body":"Create a new file segment.yaml and paste the following workflow into it:\nsegment.yaml\nThe workflow first applies the tiling service tile to the input image. The service writes its results into a directory. The segmentation service segment is then applied in a for-each action to each image tile in this directory. Steep will create a new process chain for each image tile and execute them in parallel if possible.\ntile\ntile\nsegment\nsegment\ni\ni\nmask_image\nmask_image\ntiles\ntiles\n/data/dop20_32_475_5524_1_he.jpg\n/data/dop2..."},{"slug":"submit-segmentation-workflow","body":"Run the following command to submit the segmentation workflow to Steep:\nTerminal\nYou can monitor the workflow execution in Steepâ€™s web UI. Note that it will take several minutes to complete.\nAlso, if you havenâ€™t done so already, the Docker images need to be pulled first. They have a total download size of about 8 GB, so depending on your Internet connection, downloading also may take a few minutes. Subsequent runs will be faster.\nAfter the execution has finished, you will find the segmentation masks at /data/out/[WORKFLOW ID]."},{"slug":"enable-gpu-acceleration","body":"If your machine has an NVIDIA graphics card, you can optionally enable GPU acceleration to tremendously speed up processing. In fact, this is recommended for any AI workflow in production.\nTo do so, the NVIDIA Container Toolkit needs to be installed on your system. Follow the installation guide if you havenâ€™t done so already.\nModify the metadata of the segmentation service and add the following runtime argument:\nconf/services/services.yaml\nThis will tell Steep to pass the argument --gpus all to the docker run command when it starts the segmentation service.\nImportant: A single GPU cannot be shared between multiple instances of the segmentation service. If you want to use GPU acceleration, you have to either disable parallelization or run multiple instances of Steep distributed across several machines."},{"slug":"aerial-image-segmentation","body":"Advanced tutorials\nLearn how to execute a real-world workflow that performs AI-based semantic segmentation of aerial images.\nThis tutorial assumes that youâ€™ve already installed Steep and that you know how to submit a workflow.\nLoops\nBring your own service\n"},{"slug":"agents","body":"Data models\nAn agent represents an instance of Steep that can execute process chains.\nPropertyTypeDescriptionid\n(required) string A unique agent identifier available\n(required) boolean true if the agent is currently idle and new process chains can be assigned to it, false if it is busy executing a process chain capabilities\n(required) array A set of strings specifying capabilities the agent provides. See also setups. startTime\n(required) string An ISO 8601 timestamp denoting the date and time when the agent has started. stateChangedTime\n(required) string An ISO 8601 timestamp denoting the date and time when the value of the available property changed from true to false (i.e. when the agent became busy) or from false to true (when it became available) processChainId\n(optional) string The ID of the process chain currently allocated to this agent. May be null if no process chain has been allocated to the agent (i.e. if it is currently available). \nTimeouts and retries\nVMs\n"},{"slug":"write-a-service-from-scratch","body":"We use the tiling service as an example, because itâ€™s relatively small and easy to understand. Also, it has been written in JavaScript, which makes it easy to create executable scripts.\nIn general, you can use any programming language to create processing services. Anything, thatâ€™s executable can be integrated into Steep.\nCreate a new directory, change into it, and initialize a new npm package:\nTerminal\nInstall the only dependency for the service:\nTerminal\nThen, create a new file tile.js in your directory and paste the following code into it:\ntile.js\nThe service accepts three command line arguments: the name of the image to tile, the output directory where to store the tiles, and the number of columns/rows. The main function tile creates the output directory, reads the input file, and then creates the tiles.\nNote, in case of an error, the service fails with an exit code of 1. This is important so the workflow execution will be aborted by Steep and the error wonâ€™t be suppressed.\nAlso, the service follows all guidelines from the section on processing services.\nThe script contains a Shebang at the beginning, which tells your systemâ€™s program loader to execute it through Node.js.\nMake the service executable as follows:\nTerminal\nFinally, open the file conf/services/services.yaml in your Steep installation and add the following metadata to register the service:\nconf/services/services.yaml\nReplace tile.js in the path attribute with the absolute path to tile.js on your system.\nThatâ€™s it! ðŸŽ‰ Youâ€™ve created your first service from scratch and integrated into Steep. It can now be called via its ID as seen in the previous tutorials."},{"slug":"make-the-tiling-service-executable","body":"The script contains a Shebang at the beginning, which tells your systemâ€™s program loader to execute it through Node.js.\nMake the service executable as follows:\nTerminal\n"},{"slug":"add-tiling-service-metadata","body":"Finally, open the file conf/services/services.yaml in your Steep installation and add the following metadata to register the service:\nconf/services/services.yaml\nReplace tile.js in the path attribute with the absolute path to tile.js on your system.\nThatâ€™s it! ðŸŽ‰ Youâ€™ve created your first service from scratch and integrated into Steep. It can now be called via its ID as seen in the previous tutorials."},{"slug":"create-a-docker-image","body":"In a production environment, youâ€™ll probably want to convert your custom service into a Docker image. This makes it easier to deploy the service (including all its dependencies) to multiple machines in your environment.\nCreate a new Dockerfile in the directory of the tiling service youâ€™ve created above and paste the following code into it:\nDockerfile\ndockerfile\nThe Dockerfile is very short in this case. It just includes the base image for Node.js, copies the source code, and installs the dependencies.\nWe set the entrypoint to [\"node\", \"./tile.js\"], so Steep can later simply append the service arguments to the docker run command when it creates the container.\nRun the following command to build the Docker image for the tiling service:\nTerminal\nFinally, modify the service metadata youâ€™ve added above and change the path to the name of the built Docker image tiling-service. Also change runtime to docker.\nconf/services/services.yaml\nIf you want to deploy your image to multiple machines in your distributed environment, youâ€™ll most likely want to push your image into a Docker registry. Please refer to the Docker documentation for more information."},{"slug":"build-the-docker-image","body":"Run the following command to build the Docker image for the tiling service:\nTerminal\n"},{"slug":"modify-the-service-metadata","body":"Finally, modify the service metadata youâ€™ve added above and change the path to the name of the built Docker image tiling-service. Also change runtime to docker.\nconf/services/services.yaml\nIf you want to deploy your image to multiple machines in your distributed environment, youâ€™ll most likely want to push your image into a Docker registry. Please refer to the Docker documentation for more information."},{"slug":"bring-your-own-service","body":"Advanced tutorials\nIn this tutorial, youâ€™ll learn how to create a custom processing service and integrate it into Steep. Weâ€™ll use the image tiling service from the previous tutorial as an example. So far, weâ€™ve relied on the pre-built Docker image, but what if you had to write the service from scratch or create your own Docker image?\nAerial image segmentation\nWorkflows\n"},{"slug":"custom-runtime-environments","body":"Extending Steep through plugins\nA runtime plugin is a function that can run process chain executables inside a certain runtime environment. See the runtime property of service metadata.\nThe pluginâ€™s function takes an executable to run and an output collector. The output collector is a simple interface, to which the executableâ€™s standard output should be forwarded. If required, the function can be a suspend function.\nUse this plugin if you want to implement a special way to execute processing services. For example, you can implement a remote web service call, or you can use one of the existing runtimes and run a certain service in a special way (like in the example plugin below).\nTyperuntime \nAdditional propertiesTypesupportedRuntime\n(required) string The name of the runtime this plugin provides. Use this value in your service metadata. \nFunction interface\nExample descriptor (Source)\nExample plugin script (Source)\nProcess chain consistency checkers\nProgress estimators\n"},{"slug":"query-language","body":"The focus of Steepâ€™s query language is on end users. It is lightweight and contains only a few operators. There are no logical combinators such as AND or OR. The main idea is that Steep tries to find as many objects matching the query as possible and then sorts them by relevance, so the best matches are on the top of the list. It is assumed that workflows have a higher relevance than process chains. If you are using the search through the web interface, you will typically find what you are looking for on one of the first pages. You rarely have to access any page after the second or third one. This is similar to how a web search engine works.\nA search query consists of one or more of the following elements:\nTerm: \nA string that should appear somewhere in the document to find.\nPut one or more terms into quotation marks (single or double) if you want to look for exact matches (including spaces). Within quoted terms, quotation marks can be escaped with the backslash character \\.\nEXAMPLES:\ndocker\n\"exact match\"\n\"an \\\"exact match\\\" with quotation marks\"\nDate: \nA string in the form yyyy-MM-dd\nThe operators < (less than), <= (less than or equal to), > (greater than), and >= (greater than or equal to) can be used to find objects with a date that is before or after the given one.\nEXAMPLES:\n2024-03-26\n<2024-03-26\n>=2024-03-26\nDate/time: \nA string in the form yyyy-MM-dd'T'HH:mm[:ss]. Seconds are optional.\nThe operators < (less than), <= (less than or equal to), > (greater than), and >= (greater than or equal to) can be used to find objects with a date/time pair that is before or after the given one.\nEXAMPLES:\n2024-03-26T07:16\n2024-03-26T07:16:47\n>2024-03-26T07:16\n<=2024-03-26T07:16:47\nTime range: \nA string representing a time range in the form yyyy-MM-dd['T'HH:mm[:ss]]..yyyy-MM-dd['T'HH:mm[:ss]]. Times as well as seconds within times are optional. The time range is inclusive, which means that objects with a date that matches the given start date as well as those with a date matching the given end date are included.\nEXAMPLES:\n2024-03-25..2024-03-26\n2024-03-26T07:15..2024-03-26T07:16\nLocator: \nA string starting with in: and denoting the attribute that should be compared with the give term(s), dates, date/time pairs, and time ranges. See Attributes below for a complete list of all possible attributes.\nExample: in:name\nType: \nA string starting with is: and denoting the type of documents to search. Possible values are is:workflow and is:processchain.\nFilter: \nA string that consists of an attribute name, followed by a colon and a term, date, date/time pair, or time range that should appear in this attribute. See Attributes below for a complete list of all possible attributes.\nEXAMPLES:\nname:Elvis\nstart:<=2024-03-26\nAttributes: \nPossible values (including aliases) for attributes are:\nâ€¢ id\nâ€¢ name\nâ€¢ error, errormessage\nâ€¢ rc, cap, reqcap, capability, requiredcapability, rcs, caps, reqcaps, capabilities, requiredcapabilities\nâ€¢ source\nâ€¢ start, startTime\nâ€¢ end, endTime\nSee the submission and process chain data models for more information about these attributes.\n"},{"slug":"search-examples","body":"filename highmemory: \nSearch for objects that contain the terms filename or highmemory in any of their attributes. Since results are sorted by relevance, objects matching both terms will appear at the top of the list.\n\"exact match\": \nUse quotation marks to search for literal (exact) strings.\nfilename is:workflow: \nSearch for the term filename but only in workflows.\nerror:127 is:processchain: \nSearch for process chains whose error message contains the term 127.\nhighmemory in:requiredcapabilities: \nSearch for objects where the term highmemory appears in the list of required capabilities. Does not include objects where the term highmemory appears in any other attribute but the required capabilities.\nrcs:highmemory: \nSearch for objects whose required capabilities (alias rcs) contain the term highmemory.\nfilename status:error: \nSearch for objects containing the term filename but only if their status equals error.\n2024-03-26: \nSearch for workflows or process chains that have started or finished on 26 March 2024.\n<2024-03-26T07:16: \nSearch for workflows or process chains that have started or finished before 07:16 on 26 March 2024.\n>=2024-03-26T07:16:47: \nSearch for workflows or process chains that have started or finished at or after 07:16:47 on 26 March 2024.\nstart:2024-03-25..2024-03-26: \nSearch for workflows or process chains that have been started between 25 March 2024 and 26 March 2024 (inclusive).\nfilename in:source is:workflow rcs:highmemory\n: \nSearch for workflows containing the term filename in their source and whose required capabilities contain the term highmemory.\nexit code 127 in:error is:processchain\n: \nSearch for process chains containing the terms exit, code, or 127 in their error message. Objects containing all terms in their error message will get a higher relevance and appear at the top of the list.\n"},{"slug":"search-results","body":"The HTTP endpoint /search returns a list of search result objects. Each such object refers to an item found and contains information about which properties of this item have matched with which search term and where in these properties the search term has been found.\nThe following list describes the properties of this object. For more information about each individual property, please refer to the description of the submission and process chain data models.\nPropertyTypeDescriptionid string The ID of the found item type string The itemâ€™s type. Possible values are workflow and processChain. requiredCapabilities array The capabilities required for this item to be executed. status string The current status of the item name\n(optional) string An optional human-readable name (only included if type is workflow and if the workflow has a name) startTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the item was started. May be null if the execution has not started yet. endTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the execution of the item has finished. May be null if the execution has not finished yet. matches array A list of match objects denoting which properties have matched and what term was found at which location. \n"},{"slug":"matches","body":"A match object specifies a which property of an item has matched with which search term and where the match has occurred.\nPropertyTypeDescriptionlocator string The property in which the match was found. Possible values are errorMessage, id, name, requiredCapabilities, source, status, startTime, endTime. See the description of the submission and process chain data models for more information about these properties. fragment string A fragment of the propertyâ€™s value (an â€˜excerptâ€™ or a â€˜previewâ€™). If the propertyâ€™s value is small enough, the fragment might even contain the whole value. termMatches array A list of matches within fragment (see below) \ntermMatches is an array of objects with the following properties:\nPropertyTypeDescriptionterm string The matched term from the search query indices array The start positions (relative to fragment) at which the term was found \n"},{"slug":"full-text-search","body":"Data models\nSteep contains a powerful full-text search engine that be used to find submissions and process chains in the backend database. The search engine can be accessed through the /search HTTP endpoint or through the web-based user interface.\nSetups\nHTTP endpoints\n"},{"slug":"workflow-scheduling","body":"In literature, a workflow is typically represented by a directed graph that describes how an input data set is processed by certain tasks in a given order to produce a desired outcome. The following figure shows a simple example in the extended Petri Net notation proposed by van der Aalst and van Hee (2004).\nC\nC\nA\nA\nD\nD\nB\nB\nE\nE\nThe workflow starts with an input file that is read by a task A. This task produces two results. The first one is processed by task B whose result is in turn sent to C. The second result of A is processed by D. The outcomes of C and D are finally processed by task E. This is a very simple example. In practice, workflows can become very large with hundreds up to several thousands of tasks processing large numbers of input files.\nIn order to be able to schedule such a workflow in a distributed environment, the graph has to be transformed to individual executable units. Steep follows a hybrid scheduling approach that applies heuristics on the level of the workflow graph and later on the level of individual executable units. It assumes that tasks that access the same data should be executed on the same machine to reduce the communication overhead and to improve file reuse. Steep therefore groups tasks into so-called process chains, which are linear sequential lists (without branches and loops).\nTransforming workflows into process chains is an iterative process. In each iteration, Steep finds the longest linear sequences of tasks and groups them to process chains. The following animation shows how this works for our example workflow:\nC\nC\nA\nA\nD\nD\nB\nB\nE\nE\nTask A will be put into a process chain in iteration 1. Steep then schedules the execution of this process chain. After the execution has finished, Steep uses the results to produce a process chain containing B and C and another one containing D. These process chains are then scheduled to be executed in parallel. The results are finally used to generate the fourth process chain containing task E, which is also scheduled for execution."},{"slug":"software-architecture","body":"The following figure shows the main components of Steep: the HTTP server, the controller, the scheduler, the agent, and the cloud manager.\nInstance n\nInstance n\nInstance 1\nInstance 1\nAgent\nAgent\nCloud manager\nCloud manager\nScheduler\nScheduler\nController\nController\nHTTP server\nHTTP server\nAgent\nAgent\nCloud manager\nCloud manager\nScheduler\nScheduler\nController\nController\nHTTP server\nHTTP server\nEvent bus\nEvent bus\nDatabase\nDatabase\nTogether, these components form an instance of Steep. In practice, each instance typically runs on a separate virtual machine, but multiple instances can also be started on the same machine. Each component can be enabled or disabled in a given instance (see configuration options for more information). This means, in a cluster, there can be instances that have all five components enabled, and others that only have an agent, for example.\nAll components of all instances communicate with each other through messages sent over an event bus. Further, the HTTP server, the controller, and the scheduler are able to connect to a shared database.\nThe HTTP server provides information about scheduled, running, and finished workflows to clients. Clients can also upload a new workflow. In such a case, the HTTP server puts the workflow into the database and sends a message to one of the instances of the controller.\nThe controller receives this message, loads the workflow from the database, and starts transforming it iteratively to process chains as described above. Whenever it has generated new process chains, it puts them into the database and sends a message to all instances of the scheduler.\nThe schedulers then select agents to execute the process chains. They load the process chains from the database, send them via the event bus to the selected agents for execution, and finally write the results into the database. The schedulers also send a message back to the controller so it can continue with the next iteration and generate more process chains until the workflow has been completely transformed.\nIn case a scheduler does not find an agent suitable for the execution of a process chain, it sends a message to the cloud manager (a component that interacts with the API of the Cloud infrastructure) and asks it to create a new agent."},{"slug":"processing-services","body":"Steep is very flexible and allows a wide range of processing services (or microservices) to be integrated. A typical processing service is a program that reads one or more input files and writes one or more output files. The program may also accept generic parameters. The service can be implemented in any programming language (as long as the binary or script is executable on the machine on which the Steep agent is running) or can be wrapped in a Docker container.\nFor a seamless integration, a processing service should adhere to the following guidelines:\nâ€¢ Every processing service should be a microservice. It should run in its own process and serve one specific purpose.\nâ€¢ As Steep needs to call the service in a distributed environment, it should not have a graphical user interface or require any human interaction during the runtime. Suitable services are command-line applications that accept arguments to specify input files, output files, and parameters.\nâ€¢ The service should read from input files, process the data, write results to output files, and then exit. It should not run continuously like a web service. If you need to integrate a web service in your workflow, we recommend using the curl command or something similar.\nâ€¢ Steep does not require the processing services to implement a specific interface. Instead, the serviceâ€™s input and output parameters should be described in a special data model called service metadata.\nâ€¢ According to common conventions for exit codes, a processing service should return 0 (zero) upon successful execution and any number but zero in case an error has occurred (e.g. 1, 2, 128, 255, etc.).\nâ€¢ In order to ensure deterministic workflow executions, services should be stateless and idempotent. This means that every execution of a service with the same input data and the same set of parameters should produce the same result."},{"slug":"how-does-steep-work","body":"Introduction\nTo answer this question, we will first describe how Steep transforms workflow graphs into executable units. After that, we will have a look at Steepâ€™s software architecture and what kind of processing services it can execute.\nThis guide is based on the following publication: KrÃ¤mer, M. (2020). Capability-Based Scheduling of Scientific Workflows in the Cloud. Proceedings of the 9th International Conference on Data Science, Technology, and Applications DATA, 43â€“54. https://doi.org/10.5220/0009805400430054\nGet started\nYour first workflow\n"},{"slug":"get-information","body":"Get information about Steep. This includes:\nâ€¢ Steepâ€™s version number\nâ€¢ A build ID\nâ€¢ A SHA of the Git commit for which the build was created\nâ€¢ A timestamp of the moment when the build was created\nResource URL\nParametersNone  \nStatus codes200 The operation was successful \nExample request\nExample response\n"},{"slug":"get-health","body":"An endpoint that can be used by external applications at regular intervals to monitor Steepâ€™s health, i.e. to check if it works correctly and if the underlying database as well as all remote agents are reachable.\nThe response is a JSON object, which will always contain the attribute health. This attribute can be either true if Steep works correctly or false if some error has occurred. Apart from that, the response may contain additional information about data stored in the database (such as the number of submissions) as well as information about remote agents. However, these attributes are not defined and may change in future versions.\nResource URL\nParametersNone  \nStatus codes200 The operation was successful 503 Service unavailable. Steep may not work as expected. \nExample request\nExample response\n"},{"slug":"get-submissions","body":"Get information about all submissions in the database. The response is a JSON array consisting of submission objects without the properties workflow, results, and errorMessage. In order to get the complete details of a submission, use the GET submission by ID endpoint.\nThe submissions are returned in the order in which they were added to the database with the newest ones at the top.\nResource URL\nParameterssize\n(optional) The maximum number of submissions to return. The default value is 10. offset\n(optional) The offset of the first submission to return. The default value is 0. status\n(optional) If this parameter is defined, Steep will only return submissions with the given status. Otherwise, it will return all submissions from the database. See the list of submission statuses for valid values. \nResponse headersx-page-size The size of the current page (i.e. the maximum number of submission objects returned). See size request parameter. x-page-offset The offset of the first submission returned. See offset request parameter x-page-total The total number of submissions in the database matching the given request parameters. \nStatus codes200 The operation was successful 400 One of the parameters was invalid. See response body for error message. \nExample request\nExample response\n"},{"slug":"get-submission-by-id","body":"Get details about a single submission from the database.\nResource URL\nParametersid The ID of the submission to return \nStatus codes200 The operation was successful 404 The submssion was not found \nExample request\nExample response\n"},{"slug":"put-submission","body":"Update a submission. The request body is a JSON object with the submission properties to update. At the moment, only the status and priority properties can be updated.\nIf the operation was successful, the response body contains the updated submission without the properties workflow, results, and errorMessage.\nNotes:\nâ€¢ You can use this endpoint to cancel the execution of a submission (see example below).\nâ€¢ If you update a submissionâ€™s priority, the priorities of all process chains that belong to this submission and have a status of either REGISTERED, RUNNING, or PAUSED will be updated too. Also, all process chains generated from this submission in the future will receive the updated priority. Finished process chains will not be modified.\nResource URL\nParametersid The ID of the submission to update \nStatus codes200 The operation was successful 400 The request body was invalid 404 The submission was not found \nExample request\nExample response\n"},{"slug":"post-workflow","body":"Create a new submission. The request body contains the workflow to execute.\nIf the operation was successful, the response body contains submission.\nResource URL\nStatus codes202 The workflow has been accepted (i.e. stored in the database) and is scheduled for execution. 400 The posted workflow was invalid. See response body for more information. \nExample request\nExample response\n"},{"slug":"get-process-chains","body":"Get information about all process chains in the database. The response is a JSON array consisting of process chain objects without the properties executables, results, totalRuns, runNumber, and autoResumeAfter. In order to get the complete details of a process chain, use the GET process chain by ID endpoint.\nThe process chains are returned in the order in which they were added to the database with the newest ones at the top.\nSome properties such as startTime, status, or endTime depend on a specific process chain run (see the section on process chains for more information about runs). This endpoint always displays the latest run (if available). You can get more information through the endpoints /processchains/:id/runs and /processchains/:id/runs/:runNumber.\nResource URL\nParameterssize\n(optional) The maximum number of process chains to return. The default value is 10. offset\n(optional) The offset of the first process chain to return. The default value is 0. submissionId\n(optional) If this parameter is defined, Steep will only return process chains from the submission with the given ID. Otherwise, it will return process chains from all submissions. If there is no submission with the given ID, the result will be an empty array. status\n(optional) If this parameter is defined, Steep will only return process chains with the given status. Otherwise, it will return all process chains from the database. See the list of process chain statuses for valid values. \nResponse headersx-page-size The size of the current page (i.e. the maximum number of process chain objects returned). See size request parameter. x-page-offset The offset of the first process chain returned. See offset request parameter x-page-total The total number of process chains in the database matching the given request parameters. \nStatus codes200 The operation was successful 400 One of the parameters was invalid. See response body for error message. \nExample request\nExample response\n"},{"slug":"get-process-chain-by-id","body":"Get details about a single process chain from the database.\nSome properties such as startTime, status, or endTime depend on a specific process chain run (see the section on process chains for more information about runs). This endpoint always displays the latest run (if available). You can get more information through the endpoints /processchains/:id/runs and /processchains/:id/runs/:runNumber.\nResource URL\nParametersid The ID of the process chain to return \nStatus codes200 The operation was successful 404 The process chain was not found \nExample request\nExample response\n"},{"slug":"get-process-chain-logs","body":"Get contents of the log file of a process chain.\nThis endpoint will only return something if process chain logging is enabled in the log configuration. Otherwise, the endpoint will always return HTTP status code 404.\nAlso note that process chain logs are stored on the machine where the Steep agent that has executed the process chain is running. The log files will only be available as long as the machine exists and the agent is still available. If you want to persist log files, define a location on a shared file system in the log configuration.\nThis endpoint supports HTTP range requests, which allows you to only fetch a portion of a process chainâ€™s log file.\nResource URL\nParametersid The ID of the process chain whose log file to return forceDownload\n(optional) true if the Content-Disposition header should be set in the response. This is useful if you link to a log file from a web page and want the browser to download the file instead of displaying its contents. The default value is false. runNumber\n(optional) Specifies the actual run of the process chain whose log file to return (see the section on process chains for details on runs). If no run number is given, the endpoint will return the log file of the latest run. Note that a process chain that currently has the status PAUSED or REGISTERED, does not have a latest run. In this case, the endpoint will return HTTP status code 404. \nRequest headersRange\n(optional) The part of the log file that should be returned (see HTTP Range header) \nResponse headersAccept-Ranges A marker to advertise support of range requests (see HTTP Accept-Ranges header) Content-Range\n(optional) Indicates what part of the log file is delivered (see HTTP Content-Range header) \nStatus codes200 The operation was successful 206 Partial content will be delivered in response to a range request 400 The given run number is invalid (either not a number or less than 1) 404 a) Process chain log file could not be found. Possible reasons: (1) the process chain has not produced any output (yet), (2) the agent that has executed the process chain is not available anymore, (3) process chain logging is disabled in Steepâ€™s configuration, (4) the process chain has the status PAUSED or REGISTERED, (5) the given run number was out of range 416 Range not satisfiable \nExample request\nExample response\n"},{"slug":"get-process-chain-runs","body":"Return an array of all runs of a process chain.\nA run is represented by an object containing a subset of the properties of a process chain, namely agentId, startTime, endTime, status, errorMessage, and autoResumeAfter. See the section on process chains for more information about runs and these properties.\nRuns are ordered by their run number from 1 to n where n is the total number of runs (see process chain property totalRuns).\nResource URL\nParametersid The ID of the process chain whose runs should be returned \nStatus codes200 The operation was successful 404 The process chain was not found \nExample request\nExample response\n"},{"slug":"get-process-chain-run-by-run-number","body":"Fetches information about a run of a process chain.\nThe path parameter runNumber specifies the number of the run to return. Run numbers start at 1 in continuously increasing order. The process chain property totalRuns specifies how many runs the process chain has.\nThis endpoint renders a view of a process chain for the run with the given runNumber. It renders objects similar to the ones returned by the /processchains/:id endpoint. If runNumber equals totalRuns, both endpoints return the exact same object (because /processchains/:id always renders the latest run).\nResource URL\nParametersid The ID of the process chain whose runs should be returned runNumber The number of the run to render \nStatus codes200 The operation was successful 400 The run number was invalid (i.e. not a number) 404 Either the process chain or the run were not found \nExample request\nExample response\nSee /processchains/:id endpoint."},{"slug":"head-process-chain-logs","body":"This endpoint can be used to check if a process chain log file exists and how large it is. For more information, please refer to the GET process chain logs endpoint.\nResource URL\nParametersid The ID of the process chain whose log file to check \nResponse headersContent-Length The total size of the log file Accept-Ranges A marker to advertise support of range requests (see HTTP Accept-Ranges header) \nStatus codes200 The operation was successful 404 Process chain log file could not be found. Possible reasons: (1) the process chain has not produced any output (yet), (2) the agent that has executed the process chain is not available anymore, (3) process chain logging is disabled in Steepâ€™s configuration \nExample request\nExample response\n"},{"slug":"put-process-chain","body":"Update a process chain. The request body is a JSON object with the process chain properties to update. At the moment, only the status and priority properties can be updated.\nIf the operation was successful, the response body contains the updated process chain without the properties executables and results.\nNotes:\nâ€¢ You can use this endpoint to cancel the execution of a process chain (see example below).\nâ€¢ The priority can only be modified if the process chain status is REGISTERED, RUNNING, or PAUSED. Otherwise, the operation will result in HTTP error 422.\nResource URL\nParametersid The ID of the process chain to update \nStatus codes200 The operation was successful 400 The request body was invalid 404 The process chain was not found 422 The process chainâ€™s priority could not be modified because the process chain is already finished, i.e. the process chain status is not REGISTERED, RUNNING, or PAUSED \nExample request\nExample response\n"},{"slug":"get-agents","body":"Get information about all agents currently connected to the cluster. In order to get details about a single agent, use the GET agent by ID endpoint.\nResource URL\nParametersNone  \nStatus codes200 The operation was successful \nExample request\nExample response\n"},{"slug":"get-agent-by-id","body":"Get details about a single agent.\nResource URL\nParametersid The ID of the agent to return \nStatus codes200 The operation was successful 404 The agent was not found \nExample request\nExample response\n"},{"slug":"get-vms","body":"Get information about all VMs in the database. To get details about a single VM, use the GET VM by ID endpoint.\nThe VMs are returned in the order in which they were added to the database with the newest ones at the top.\nResource URL\nParameterssize\n(optional) The maximum number of VMs to return. The default value is 10. offset\n(optional) The offset of the first VM to return. The default value is 0. status\n(optional) If this parameter is defined, Steep will only return VMs with the given status. Otherwise, it will return all VMs from the database. See the list of VM statuses for valid values. \nResponse headersx-page-size The size of the current page (i.e. the maximum number of VM objects returned). See size request parameter. x-page-offset The offset of the first VM returned. See offset request parameter x-page-total The total number of VMs in the database matching the given request parameters. \nStatus codes200 The operation was successful 400 One of the parameters was invalid. See response body for error message. \nExample request\nExample response\n"},{"slug":"get-vm-by-id","body":"Get details about a single VM from the database.\nResource URL\nParametersid The ID of the VM to return \nStatus codes200 The operation was successful 404 The VM was not found \nExample request\nExample response\n"},{"slug":"get-plugins","body":"Get information about all configured plugins. To get information about a single plugin, use the GET plugin by name endpoint.\nResource URL\nParametersNone  \nStatus codes200 The operation was successful \nExample request\nExample response\n"},{"slug":"get-plugin-by-name","body":"Get information about a single plugin.\nResource URL\nParametersname The name of the plugin to return \nStatus codes200 The operation was successful 404 The plugin was not found \nExample request\nExample response\n"},{"slug":"get-services","body":"Get information about all configured service metadata. To get metadata of a single service, use the GET service by ID endpoint.\nResource URL\nParametersNone  \nStatus codes200 The operation was successful \nExample request\nExample response\n"},{"slug":"get-service-by-id","body":"Get configured metadata of a single service.\nResource URL\nParametersid The ID of the service to return \nStatus codes200 The operation was successful 404 The service metadata was not found \nExample request\nExample response\n"},{"slug":"get-setups","body":"Get information about all configured setups. To get a single setup, use the GET setup by ID endpoint.\nResource URL\nParametersNone  \nStatus codes200 The operation was successful 404 Setups are unavailable because cloud configuration is disabled \nExample request\nExample response\n"},{"slug":"get-setup-by-id","body":"Get information about a single setup.\nResource URL\nParametersid The ID of the setup to return \nStatus codes200 The operation was successful 404 The setup cannot be found or setups are unavailable because cloud configuration is disabled \nExample request\nExample response\n"},{"slug":"get-search","body":"Perform a full-text search in Steepâ€™s database to find submissions and process chains.\nThe response is a JSON object with the attributes counts and results:\nâ€¢ counts is an object containing the total number of matches in the database for each possible object type (workflow and processChain) as well as a total sum (total). The numbers may either be exact or estimated depending on the count parameter given in the request. The object might also be left off if count equals none.\nâ€¢ results is a list of search result objects sorted by relevance.\nResource URL\nParametersq The search query size\n(optional) The maximum number of search results to return. The default value is 10. offset\n(optional) The offset of the first search result to return. The default value is 0. count\n(optional) Specifies how Steep should count the total number of search results and if they should be included in the response. Possible values are: exact to include an exact count in the response (may take a long time depending on how many objects match), estimate to include a rough estimate (very fast but might be incorrect), and none if the total number of results should not be counted at all. The default value is estimate. timeZone\n(optional) An ID of the time zone in which dates and times in the query are specified (typically the time zone of the client or web browser). Valid values are taken from the IANA Time Zone database (e.g. Europe/Berlin). Defaults to the serverâ€™s time zone. \nStatus codes200 The operation was successful 400 One of the parameters was invalid. See response body for error message. \nExample request\nExample response\n"},{"slug":"get-prometheus-metrics","body":"Steep can provide metrics to Prometheus. Besides statistics about the Java Virtual Machine that Steep is running in, the following metrics are included:\nMetricDescriptionsteep_remote_agents The number of registered remote agents steep_controller_process_chains The number of generated process chains the controller is currently waiting for grouped by submissionId steep_scheduler_process_chains The number of process chains with a given status (indicated by the label status) steep_local_agent_retries The total number of times an executable with a given service ID (indicated by the label serviceId) had to be retried steep_eventbus_compressedjson_messages Total number of compressed JSON messages sent/received over the event bus (label operation is either sent or recv) steep_eventbus_compressedjson_bytes Total number of compressed JSON bytes. Label operation is either sent or recv, and label stage is either compressed or uncompressed. Uncompressed bytes will be compressed before they are sent over the event bus, and received compressed bytes will be uncompressed. steep_eventbus_compressedjson_time Total number of milliseconds spent compressing or decompressing JSON (label operation is either sent or recv and specifies if sent bytes have been compressed or received bytes have been uncompressed) \nResource URL\nParametersNone  \nStatus codes200 The operation was successful \nExample request\nExample response\n"},{"slug":"http-endpoints","body":"Interfaces\nThe main way to communicate with Steep (i.e. to submit workflows, to monitor progress, fetch metadata, etc.) is through its HTTP interface. By default, Steep listens to incoming connections on port 8080.\nFull-text search\nWeb-based user interface\n"},{"slug":"initializers","body":"Extending Steep through plugins\nAn initializer plugin is a function that will be called during the initialization phase of Steep just before components such as the controller or the scheduler are deployed. If required, the function can be a suspend function.\nTypeinitializer \nAdditional propertiesTypedependsOn\n(optional) array An optional list of names of other initializer plugins that need to be executed before this plugin. If this property is not given, the order in which initializer plugins are executed is undefined. \nFunction interface\nExample descriptor\nExample plugin script\nOverview\nOutput adapters\n"},{"slug":"create-a-countdown-service","body":"To demonstrate how loops work in Steep, we create a workflow that counts a number down until it has reached 0. The workflow uses a for-each action to repeatedly call a service that reads a number from a file, decreases it, and then writes the new value to an output file. After each service call, the output is fed back into the for-each actionâ€™s input, which makes Steep call the service again and again.\nTo end the loop, we just need to make sure that no more items are appended to the for-each actionâ€™s input list. When the service reaches the value 0, it therefore does not produce a new output file.\nTo implement the service, we use Node.js. Create a new file countdown.js and paste the following code into it:\ncountdown.js\nNote that we could use any programming language to implement this service. Weâ€™re just using Node.js here because it makes it very easy to create executable scripts. Read the tutorial on bringing your own service for a more advanced example."},{"slug":"make-the-service-executable","body":"The countdown script contains a Shebang at the beginning, which tells your systemâ€™s program loader to execute it through Node.js.\nYou just need to make it executable as follows:\nTerminal\n"},{"slug":"add-countdown-metadata","body":"Next, we need to define the metadata for our new service. Open the file conf/services/services.yaml and add the following code:\nconf/services/services.yaml\nChange the path attribute to the absolute location of the countdown.js file on your system.\nNote that we use the data type fileOrEmptyList for the serviceâ€™s output parameter. This is a special data type that either returns the generated file or an empty list if the file does not exist. Using the data type file would lead to an error during workflow execution.\nDonâ€™t forget to restart Steep for the changes to take effect."},{"slug":"create-loop-workflow","body":"Create a new file loop-workflow.yaml and add the following code:\nloop-workflow.yaml\nIn the first iteration of the for-each action, the service reads from the file input.txt and writes to an output file with a name generated during runtime. The path of this output file is fed back into the for-each action via yieldToInput. In the second iteration, the service reads from the output file and produces another one. This process continues until the number equals 0, in which case the service does not write an output file and the workflow finishes.\ncountdown\ncountdown\noutput_file\noutput_file\ni\ni\ninput_file\ninput_file"},{"slug":"submit-loop-workflow","body":"Create a new file input.txt with the number 10 in it:\nTerminal\nEdit the file loop-workflow.yaml that youâ€™ve created in the previous step and replace the relative path to input.txt with the absolute path to this file on your computer.\nThen, submit the workflow:\nTerminal\nVisit the workflow page in Steepâ€™s web UI (http://localhost:8080/workflows) and click on the ID of your submitted workflow. After the workflow has finished, it should display ten completed process chains, one for each iteration."},{"slug":"loops","body":"Tutorials\nIn the workflows from the previous tutorials, data flowed in one direction from one action to the next until the final output was produced. However, sometimes it is necessary to feed back the results of one action into a preceding one or even into the same until a certain condition has been reached.\nIn general purpose programming languages, this concept can be modelled with while or for loops. Steep has a similar but much more powerful concept. In the previous tutorial, youâ€™ve learned about for-each actions, which apply a certain set of actions to a list of inputs. This can be used to execute multiple actions in parallel. Also, the list of inputs is dynamic. New items can be appended during workflow execution, which will make Steep automatically generate more process chains. The loop will finish once there are no more items in the input list.\nAlthough, for-each actions can be used to model loops, as described in the previous tutorial, for-each actions are not loops per se! In fact, a for-each action just applies a set of other actions to each item in its input. This can be done in parallel and in any order. The fact that you can append more items to its input list does not change this behaviour. Process chains for new items might be scheduled in parallel to existing process chains or even earlier.\nThis tutorial teaches you how to use the yieldToInput keyword to append new items to a for-each actionâ€™s input list during workflow execution.\nParallelization\nAerial image segmentation\n"},{"slug":"macro-parameters","body":"Macro parameters describe inputs and outputs of macros.\nPropertyTypeDescriptionid\n(required) string A unique parameter identifier name\n(required) string A human-readable name description\n(required) string A human-readable description type\n(required) string The type of this parameter. Valid values: input, output default\n(optional) string An optional default value for this parameter that will be used if a parameter value is not explicitly provided when the macro is included. \n"},{"slug":"macros","body":"Data models\nMacros can be used to model reusable workflow parts. A macro consists of a list of actions and has input and output parameters. It can be included in a workflow (or indeed another macro) through an include action.\nMacros are specified in the configuration file macros/macros.yaml.\nPropertyTypeDescriptionid\n(required) string A unique macro identifier name\n(required) string A human-readable name description\n(required) string A human-readable description parameters\n(optional) array An optional list of input and output parameters that can be accessed from outside the macro to provide arguments or to access return values, respectively. vars\n(optional) array An optional list of private variables that can be used inside the macro. These variables are not accessible from the outside. actions\n(required) array The actions to execute \nService metadata\nTimeouts and retries\n"},{"slug":"output-adapters","body":"Extending Steep through plugins\nAn output adapter plugin is a function that can manipulate the output of services depending on their produced data type (see the dataType property of the service parameter data model, as well as the dataType property of the process chain argument data model).\nIn other words, if an output parameter of a processing service has a specific dataType defined in the serviceâ€™s metadata and this data type matches the one given in the output adapterâ€™s descriptor, then the pluginâ€™s function will be called after the service has been executed. Steep will pass the output argument and the whole process chain (for reference) to the plugin. The output argumentâ€™s value will be the current result (i.e. the output file or directory). The plugin can modify this file or directory (if necessary) and return a new list of files that will then be used by Steep for further processing.\nSteep has a built-in output adapter for the data type directory. Whenever you specify this data type in the service metadata, Steep will pass the output directory to an internal function that recursively collects all files from this directory and returns them as a list.\nThe example output adapter fileOrEmptyList described below is also included in Steep. It checks if an output file exists (i.e. if the processing service has actually created it) and either returns a list with a single element (the file) or an empty list. This is useful if a processing service has an optional output that you want to use as input of a subsequent for-each action or of the current for-each action via yieldToInput.\nIf required, the function can be a suspend function.\nTypeoutputAdapter \nAdditional propertiesTypesupportedDataType\n(required) string The dataType that this plugin can handle \nFunction interface\nExample descriptor\nExample plugin script\nInitializers\nProcess chain adapters\n"},{"slug":"configure-steep-for-parallelization","body":"In order to enable parallelization, you either have to configure multiple agents or launch more than one Steep instance. The first approach allows you to run multiple process chains in parallel on the same machine (vertical scaling), while the second one enables horizontal scaling across several machines in a distributed environment. Both approaches can be combined.\nBy default, every Steep instance spawns only one agent. You can configure the number of agents in the general configuration file conf/steep.yaml. For example, the following configuration will spawn 12 agents:\nconf/steep.yaml\nRestart Steep for the changes to take effect. Then, open the â€˜Agentsâ€™ tab in Steepâ€™s UI to see the list of agents: http://localhost:8080/agents\nYou may also access Steepâ€™s HTTP API:\nTerminal\nSteep comes with a built-in clustering solution based on Eclipse Vert.x and Hazelcast.\nIf you run multiple instances of Steep on your local machine, they will automatically form a cluster. For this to work, you have to change the ports for the event bus and the HTTP server of each instance. The default configuration can be overridden through environment variables.\nAfter downloading the Steep distribution, open your terminal and change into the directory where youâ€™ve extracted it. Run the following command:\nTerminal\nRun this command as many times as you want but make sure to change the values of STEEP_CLUSTER_EVENTBUS_PORT, STEEP_CLUSTER_EVENTBUS_PUBLICPORT, and STEEP_HTTP_PORT for each instance. For example:\nTerminal\nThe default configuration will also allow you to run Steep on multiple machines in the same network. The instances will automatically detect each other (through multicasting) and form a cluster. If this does not work (because your firewall does not allow multicasting) or if you need a more advanced distributed deployment across different networks or with specific IP addresses, read the section on cluster settings."},{"slug":"spawn-multiple-agents","body":"By default, every Steep instance spawns only one agent. You can configure the number of agents in the general configuration file conf/steep.yaml. For example, the following configuration will spawn 12 agents:\nconf/steep.yaml\nRestart Steep for the changes to take effect. Then, open the â€˜Agentsâ€™ tab in Steepâ€™s UI to see the list of agents: http://localhost:8080/agents\nYou may also access Steepâ€™s HTTP API:\nTerminal\n"},{"slug":"launch-multiple-steep-instances","body":"Steep comes with a built-in clustering solution based on Eclipse Vert.x and Hazelcast.\nIf you run multiple instances of Steep on your local machine, they will automatically form a cluster. For this to work, you have to change the ports for the event bus and the HTTP server of each instance. The default configuration can be overridden through environment variables.\nAfter downloading the Steep distribution, open your terminal and change into the directory where youâ€™ve extracted it. Run the following command:\nTerminal\nRun this command as many times as you want but make sure to change the values of STEEP_CLUSTER_EVENTBUS_PORT, STEEP_CLUSTER_EVENTBUS_PUBLICPORT, and STEEP_HTTP_PORT for each instance. For example:\nTerminal\nThe default configuration will also allow you to run Steep on multiple machines in the same network. The instances will automatically detect each other (through multicasting) and form a cluster. If this does not work (because your firewall does not allow multicasting) or if you need a more advanced distributed deployment across different networks or with specific IP addresses, read the section on cluster settings."},{"slug":"execute-parallel-workflows","body":"In the following, we present two ways to extend the workflow from the previous tutorial to download and extract multiple files at the same time.\nThis section assumes that youâ€™ve read and followed the previous tutorial and that youâ€™ve already configured the metadata for the download and extract services.\nWhile processing the workflow graph, Steep looks for independent sequences of actions and converts them to process chains. The following workflow contains four actions (two download actions and two extract actions). There are two output variables o1 and o2 that connect the services with each other.\ndownload-independent-actions.yaml\nAs described in the previous tutorial, Steep automatically detects dependencies between the actions. In this case, the first extract action depends on the output of the first download action (o1), and the second extract action depends on the second download action (o2).\ndownload\ndownload\nextract\nextract\no1\no1\nwebsite_output_directory\nwebsite_ou...\nhttps://.../steep-wms.github.io/...\nhttps://.....\ndownload\ndownload\nextract\nextract\no2\no2\nsteep_output_directory\nsteep_outp...\nhttps://.../steep/...\nhttps://.....\nBoth pairs of actions are independent. Steep converts them to different process chains, which can then be executed in parallel.\nWhile option A does exactly what we want, it is kind of verbose: The download and extract actions have to be repeated for every file. But what if we wanted to apply this to, say, a hundred or a thousand files?\nFor this purpose, Steep offers so-called for-each actions that iterate over a list of inputs and apply a set of actions to each of them. During workflow execution, Steep is able to automatically convert the individual iterations of a for-each action to independent process chains and run them in parallel.\nfor-each actions are not loops! Although, you may be used to general purpose programming languages that sometimes also have a for-each construct, which is executed with a single thread and, hence, sequentially, for-each actually just means that something should be applied to each element of a set. In our case, a set of actions is applied to each item in the for-each actionâ€™s input. This can be done in parallel and in any order.\nThe following workflow works exactly like the one from option A but uses a variable and a for-each action instead to reduce boilerplate code.\ndownload-for-each.yaml\nThe graphical representation of the workflow is as follows:\ndownload\ndownload\nextract\nextract\no\no\nhttps://.../steep-wms.github.io/...\nhttps://.....\nhttps://.../steep/...\nhttps://.....\noutput_directory\noutput_dir...\n-\n-\n-\n-\ni\ni\nurls\nurls\nWe are using a variable (urls) with a constant value in this workflow, but it is also possible to apply a for-each action to the output of a service. This is useful if a service produces a directory and you want to process each file in this directory. Specify the data type directory for the output parameter in the serviceâ€™s metadata and, after the service has finished, Steep will automatically traverse the directory recursively to collect all files in a list. We will use this approach in the advanced tutorial on aerial image segmentation."},{"slug":"independent-actions","body":"While processing the workflow graph, Steep looks for independent sequences of actions and converts them to process chains. The following workflow contains four actions (two download actions and two extract actions). There are two output variables o1 and o2 that connect the services with each other.\ndownload-independent-actions.yaml\nAs described in the previous tutorial, Steep automatically detects dependencies between the actions. In this case, the first extract action depends on the output of the first download action (o1), and the second extract action depends on the second download action (o2).\ndownload\ndownload\nextract\nextract\no1\no1\nwebsite_output_directory\nwebsite_ou...\nhttps://.../steep-wms.github.io/...\nhttps://.....\ndownload\ndownload\nextract\nextract\no2\no2\nsteep_output_directory\nsteep_outp...\nhttps://.../steep/...\nhttps://.....\nBoth pairs of actions are independent. Steep converts them to different process chains, which can then be executed in parallel."},{"slug":"using-a-for-each-action","body":"While option A does exactly what we want, it is kind of verbose: The download and extract actions have to be repeated for every file. But what if we wanted to apply this to, say, a hundred or a thousand files?\nFor this purpose, Steep offers so-called for-each actions that iterate over a list of inputs and apply a set of actions to each of them. During workflow execution, Steep is able to automatically convert the individual iterations of a for-each action to independent process chains and run them in parallel.\nfor-each actions are not loops! Although, you may be used to general purpose programming languages that sometimes also have a for-each construct, which is executed with a single thread and, hence, sequentially, for-each actually just means that something should be applied to each element of a set. In our case, a set of actions is applied to each item in the for-each actionâ€™s input. This can be done in parallel and in any order.\nThe following workflow works exactly like the one from option A but uses a variable and a for-each action instead to reduce boilerplate code.\ndownload-for-each.yaml\nThe graphical representation of the workflow is as follows:\ndownload\ndownload\nextract\nextract\no\no\nhttps://.../steep-wms.github.io/...\nhttps://.....\nhttps://.../steep/...\nhttps://.....\noutput_directory\noutput_dir...\n-\n-\n-\n-\ni\ni\nurls\nurls\nWe are using a variable (urls) with a constant value in this workflow, but it is also possible to apply a for-each action to the output of a service. This is useful if a service produces a directory and you want to process each file in this directory. Specify the data type directory for the output parameter in the serviceâ€™s metadata and, after the service has finished, Steep will automatically traverse the directory recursively to collect all files in a list. We will use this approach in the advanced tutorial on aerial image segmentation."},{"slug":"submit-parallel-workflows","body":"Submit the workflows from above to Steep and familiarize yourself with how it converts them to process chains and what directories and files it generates in the output directory.\nVisit the workflow page in Steepâ€™s web UI (http://localhost:8080/workflows) and click on the ID of your submitted workflow. It should display two process chains, one for each downloaded file.\nList the contents of Steepâ€™s output directory for this workflow:\nTerminal\nYou should find two sub-directories, one for each extract action output."},{"slug":"parallelization","body":"Tutorials\nIn the previous tutorial, youâ€™ve learned how to create a workflow that downloads a single .zip file and extracts it to Steepâ€™s output directory. We will now extend this workflow to download and extract multiple files.\nIn certain situations, it can be beneficial to perform operations in parallel. For example, running multiple downloads at the same time can improve bandwidth usage. Since a .zip file can only be extracted with a single thread, running multiple unzip instances in parallel can improve CPU utilization. Also, through horizontal scaling, CPU-intensive tasks can be distributed to multiple machines.\nThis tutorial will show you how to write workflows where the individual actions are executed in parallel. Note that you donâ€™t have to model parallelization yourself. Steep is able to automatically detect independent branches of the workflow graph (so-called process chains) and execute them in parallel. Read more about the approach to parallelization and process chain generation in How does Steep work?\nSequential workflows\nLoops\n"},{"slug":"parameter-injection","body":"For compatibility reasons, the plugin function signatures are not fixed. Whenever a plugin function is called, its arguments are injected based on their type. For example, if the function requires an instance of io.vertx.core.Vertx, such an object will be passed. This is independent of where this argument appears in the parameter list. In the function signatures described above, the Vert.x instance is always the last parameter, but actually, it could be at any position and the injection mechanism would still work.\nThis feature allows us as Steep developers to add new parameters in the future to the function signatures without requiring all users to change their plugins. It also allows plugin developers to specify parameters in any order and to even leave out parameters if they are not needed. For example, a process chain adapter could also have the following signature:\nNote that a parameter can only be injected into a plugin of a certain type, if it is described in the function signature of this plugin type above. For example, the function signature of a runtime plugin does not specify a workflow, so a workflow cannot be injected. Trying to define a runtime plugin with such a parameter will result in an error."},{"slug":"plugin-overview","body":"Extending Steep through plugins\nSteep can be extended through plugins. Each plugin is a Kotlin script with the file extension .kt or .kts. Inside this script, there should be a single function with the same name as the plugin and a signature that depends on the plugin type. Function interfaces are described in the sub-sections below.\nAll plugins must be referenced in the plugins/common.yaml file. This file is an array of descriptor objects with at least the following properties:\nPropertyTypeDescriptionname\n(required) string A unique name of the plugin (the function inside the pluginâ€™s script file must have the same name) type\n(required) string The plugin type. Valid values are: initializer, outputAdapter, processChainAdapter, processChainConsistencyChecker, and runtime. scriptFile\n(required) string The path to the pluginâ€™s Kotlin script file. The file should have the extension .kt or .kts. The path is relative to Steepâ€™s application directory, so a valid example is conf/plugins/fileOrEmptyList.kt. version\n(optional) string The pluginâ€™s version. Must follow the Semantic Versioning Specification. \nSpecific plugin types may require additional properties described in the sub-sections below.\nPlugins are compiled on demand when Steep is starting. This can take some time depending on the number of the plugins and their size. For faster start times, you can configure a persistent compiled plugin cache. Steep updates this cache on startup when it has first compiled a script or when it detects that a previously compiled script has changed. On subsequent startups, Steep can utilize the cache to skip compilation of known plugins and, therefore, to reduce startup time.\nUsing a template engine\nInitializers\n"},{"slug":"macrosmacrosyaml","body":"Configuration\nThe file macros/macros.yaml contains an array of macros that describe reusable workflow parts.\nNote: The path to this file can be configured with the item steep.macros in Steepâ€™s general configuration.\nmacros/macros.yaml\nservices/services.yaml\nplugins/common.yaml\n"},{"slug":"process-chain-adapters","body":"Extending Steep through plugins\nA process chain adapter plugin is a function that can manipulate generated process chains before they are executed.\nIt takes a list of generated process chains and a reference to the workflow from which the process chains have been generated. It returns a new list of process chains to execute or the given list if no modification was made. If required, the function can be a suspend function.\nTypeprocessChainAdapter \nAdditional propertiesTypedependsOn\n(optional) array An optional list of names of other process chain adapter plugins that need to be executed before this plugin. If this property is not given, the order in which process chain adapter plugins are applied to process chains is undefined. \nFunction interface\nExample descriptor\nExample plugin script\nOutput adapters\nProcess chain consistency checkers\n"},{"slug":"pluginscommonyaml","body":"Configuration\nThe configuration file plugins/common.yaml describes all plugins so Steep can compile and use them during runtime. The file contains an array of descriptor objects with the properties specified in the section on extending Steep through plugins.\nNote: The path to the plugin configuration file can be configured with the item steep.plugins in Steepâ€™s general configuration.\nplugins/common.yaml\nmacros/macros.yaml\nProvisioning scripts\n"},{"slug":"process-chain-consistency-checkers","body":"Extending Steep through plugins\nA process chain consistency checker plugin is a function that checks if an execute action can be added to a process chain or if adding it would lead to inconsistencies that render the process chain inexecutable.\nThe function is called before the execute action is converted to an executable and before it is added to the process chain. It takes a list of executables that have been collected so far during process chain generation, the execute action that is about to be converted, the workflow from which the process chain is being created, and a Vert.x instance.\nIt returns true if the execute action can safely be converted and added to the list of executables to make up a new process chain, or false if adding it would lead to a process chain that could not be executed later.\nSteep will not discard actions that a consistency checker plugin has rejected. Instead, it will try to add them to another process chain later. At this point, the plugin will be called again.\nIf required, the function can be a suspend function.\nTypeprocessChainConsistencyChecker \nAdditional propertiesTypedependsOn\n(optional) array An optional list of names of other process chain consistency checker plugins that need to be executed before this plugin. If this property is not given, the order in which consistency checkers are applied to process chains is undefined. \nFunction interface\nExample descriptor\nExample plugin script\nProcess chain adapters\nCustom runtime environments\n"},{"slug":"executables","body":"An executable is part of a process chain. It describes how a processing service should be executed and with which parameters.\nPropertyTypeDescriptionid\n(required) string An identifier (does not have to be unique). Typically refers to the id of the execute action, from which the executable was derived. Possibly suffixed with a dollar sign $ and a number denoting the iteration of an enclosing for-each action (e.g. myaction$1) or nested for-each actions (e.g. myaction$2$1). path\n(required) string The path to the binary of the service to be executed. This property is specific to the runtime. For example, for the docker runtime, this property refers to the Docker image. serviceId\n(required) string The ID of the processing service to be executed. arguments\n(required) array A list of arguments to pass to the service. May be empty. runtime\n(required) string The name of the runtime that will execute the service. Built-in runtimes are currently other (for any service that is executable on the target system) and docker for Docker containers. More runtimes can be added through plugins runtimeArgs\n(optional) array A list of arguments to pass to the runtime. May be empty. retries\n(optional) object An optional retry policy specifying how often this executable should be restarted in case of an error. maxInactivity\n(optional) object An optional timeout policy that defines how long the executable can run without producing any output (i.e. without writing anything to the standard output and error streams) before it is automatically cancelled or aborted. maxRuntime\n(optional) object An optional timeout policy that defines how long the executable can run before it is automatically cancelled or aborted, even if the service regularly writes to the standard output and error streams. deadline\n(optional) object An optional timeout policy that defines how long the executable can run at all (including all retries and their associated delays) until it is cancelled or aborted. \nAn argument is part of an executable.\nPropertyTypeDescriptionid\n(required) string An argument identifier label\n(optional) string An optional label to use when the argument is passed to the service (e.g. --input). variable\n(required) object A variable that holds the value of this argument. type\n(required) string The type of this argument. Valid values: input, output dataType\n(required) string The type of the argument value. If this property is directory, Steep will create a new directory for the serviceâ€™s output and recursively search it for result files after the service has been executed. Otherwise, this property can be an arbitrary string. New data types with special handling can be added through output adapter plugins. \nAn argument variable holds the value of an argument.\nPropertyTypeDescriptionid\n(required) string The variableâ€™s unique identifier value\n(required) string The variableâ€™s value \n"},{"slug":"arguments","body":"An argument is part of an executable.\nPropertyTypeDescriptionid\n(required) string An argument identifier label\n(optional) string An optional label to use when the argument is passed to the service (e.g. --input). variable\n(required) object A variable that holds the value of this argument. type\n(required) string The type of this argument. Valid values: input, output dataType\n(required) string The type of the argument value. If this property is directory, Steep will create a new directory for the serviceâ€™s output and recursively search it for result files after the service has been executed. Otherwise, this property can be an arbitrary string. New data types with special handling can be added through output adapter plugins. \n"},{"slug":"argument-variables","body":"An argument variable holds the value of an argument.\nPropertyTypeDescriptionid\n(required) string The variableâ€™s unique identifier value\n(required) string The variableâ€™s value \n"},{"slug":"process-chain-status","body":"The following table shows the statuses a process chain can have:\nStatusDescriptionREGISTERED The process chain has been created but execution has not started yet RUNNING The process chain is currently being executed PAUSED The execution of the process chain is paused CANCELLED The execution of the process chain was cancelled SUCCESS The process chain was executed successfully ERROR The execution of the process chain failed \n"},{"slug":"process-chains","body":"Data models\nAs described in the section on workflow scheduling, Steep transforms a workflow to one or more process chains. A process chain is a sequential list of instructions that will be sent to Steepâ€™s remote agents to execute processing services in a distributed environment.\nSome of the properties specified in the table below are only available once Steep has started executing a process chain (e.g. startTime) or after the execution has finished (e.g. endTime).\nAlso, the /processchains HTTP endpoint, which provides a list of process chains, omits some properties although they are marked as required in the table below (e.g. executables or totalRuns). If you want to get all required properties, you have to use the /processchains/:id HTTP endpoint.\nSteep records each execution of a process chain in a separate â€˜runâ€™. The property totalRuns specifies how often a process chain has been executed (including any currently running execution). If a process chain has just been created and still has the status REGISTERED, totalRuns equals 0, but as soon as the status switches to RUNNING, a new run is created and totalRuns is incremented to 1. If a process chain fails and is later retried, for example, a new run will be created and totalRuns will be 2, etc.\nRequesting a process chain through the HTTP endpoints /processchains or /processchains/:id, always renders the latest run. The properties agentId, startTime, endTime, status, errorMessage, and autoResumeAfter depend on the actual run rendered (e.g. different runs have different start times; or one run might have failed, while a newer one might have succeeded or is still running, so their statuses are different). If you want to list all runs of a process chain or retrieve information about a specific run, use the /processchains/:id/runs or /processchains/:id/runs/:runNumber HTTP endpoints, respectively. The property runNumber from the table below specifies, which run out of totalRuns is rendered.\nPropertyTypeDescriptionid\n(required) string Unique process chain identifier executables\n(required) array A list of executable objects that describe what processing services should be called and with which arguments submissionId\n(required) string The ID of the submission to which this process chain belongs agentId\n(optional) string The ID of the agent that currently executes the process chain (if its status is RUNNING) or has executed it (if it is finished). May be null if the execution has not started yet. startTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the process chain execution was started. May be null if the execution has not started yet. endTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the process chain execution finished. May be null if the execution has not finished yet. status\n(required) string The current status of the process chain requiredCapabilities\n(optional) array A set of strings specifying capabilities a host system must provide to be able to execute this process chain. See also setups. priority\n(optional) number A priority used during scheduling. Process chains with higher priorities will be scheduled before those with lower priorities. Negative values are allowed. The default value is 0. retries\n(optional) object An optional retry policy specifying how often this process chain will be rescheduled in case an error has occurred. results\n(optional) object If status is SUCCESS, this property contains the list of process chain result files grouped by their output variable ID. Otherwise, it is null. totalRuns\n(required) number The number of times the process chain has been executed (including any currently running execution). runNumber\n(optional) number The number of the run currently rendered. May be null if the process chain has not been executed yet. autoResumeAfter\n(optional) string If the process chainâ€™s status is PAUSED, this optional property may specify a point in time (as an ISO 8601 timestamp) after which the process chain will be automatically resumed. It is typically only given, if a retry policy is configured (see retries property): if a process chain run has failed and there are still attempts left, autoResumeAfter specifies when the next attempt will be performed. estimatedProgress\n(optional) number A floating point number between 0.0 (0%) and 1.0 (100%) indicating the current execution progress of this process chain. This property will only be provided if the process chain is currently being executed (i.e. if its status equals RUNNING) and if a progress could actually be estimated. Note that the value is an estimation based on various factors and does not have to represent the real progress. More precise values can be calculated with a progress estimator plugin. Sometimes, progress cannot be estimated at all. In this case, the value will be null. errorMessage\n(optional) string If status is ERROR, this property contains a human-readable error message. Otherwise, it is null. \nWorkflows\nSubmissions\n"},{"slug":"progress-estimators","body":"Extending Steep through plugins\nA progress estimator plugin is a function that analyses the log output of a running processing service to estimate its current progress. For example, the plugin can look for percentages or number of bytes processed. The returned value contributes to the execution progress of a process chain (see the estimatedProgress property of the process chain data model).\nThe function takes the executable that is currently being run and a list of recently collected output lines. It returns an estimated progress between 0.0 (0%) and 1.0 (100%) or null if the progress could not be determined. The function will be called for each output line collected and the newest line is always at the end of the given list. If required, the function can be a suspend function.\nTypeprogressEstimator \nAdditional propertiesTypesupportedServiceIds\n(required) array An array of IDs of the services this estimator plugin supports \nFunction interface\nExample descriptor\nExample plugin script\nCustom runtime environments\n"},{"slug":"global-variables","body":"Here is a list of the global variables defined in provisioning scripts:\nVariableDescriptionagentId The ID of the agent that is about to be deployed on the provisioned VM ipAddress The IP address of the provisioned VM setup A Setup object that describes the configuration of the provisioned VM \nExample provisioning script\n"},{"slug":"environment-variables","body":"Access any environment variable defined on the system where Steep is running (not the VM to be provisioned) as follows:\nExample provisioning script\n"},{"slug":"configuration-values","body":"Use values from Steepâ€™s main configuration file (steep.yaml) as follows:\nExample provisioning script\n"},{"slug":"read-local-files","body":"Insert the contents of a local file into the provisioning script.\nExample provisioning script\n"},{"slug":"upload-local-files-to-remote","body":"Upload one or more local files to the remote VM.\nsource may contain a glob pattern (e.g. *.sh or **/*.yaml) if you want to upload multiple files or if you want to recursively transfer a whole directory tree. In this case, destination should be a directory and it should exist.\nExample provisioning script\n"},{"slug":"provisioning-scripts","body":"Advanced configuration topics\nAfter Steepâ€™s cloud manager has created a VM, it uploads the provisioning scripts defined in the corresponding setup to the VM and executes them. The scripts can be used to deploy software to the VM and to make sure all required services (including Steep) are running.\nEach provisioning script is a simple executable script file with a valid UNIX shebang at the beginning.\nScript files will be processed by a template engine before they are uploaded. The following sections describe supported template instructions.\nplugins/common.yaml\nUsing YAML anchors\n"},{"slug":"add-more-service-metadata","body":"In this tutorial, we are going to create a workflow that downloads a .zip file from an HTTP server and then extracts it. For this, we will use the wget and unzip programs. Similar to the first tutorial, we need to add service metadata for these services to the conf/services/services.yaml file.\nFirst, add the following metadata for the wget service:\nconf/services/services.yaml\nSimilar to the cp service from the first tutorial, this service has two parameters. However, in this case, the output parameter has been specified first and has a label -O. Steep will respect the order of the parameters and the label when calling wget later during workflow execution and generate a command line in the following form:\nThe output parameter also has a fileSuffix. Specifying this will make sure Steep generates an output filename with the correct extension (in this case .zip) later during workflow execution.\nNext, we need to add the metadata for the unzip service:\nconf/services/services.yaml\nIn this case, the output parameter has a label -d and is a directory. Steep is able to automatically create output directories during workflow execution.\nDonâ€™t forget to restart Steep after changing the service metadata configuration file."},{"slug":"create-sequential-workflow","body":"Create a new file download.yaml and paste the following workflow into it:\ndownload.yaml\nThe workflow consists of two execute actions calling the download and the extract service. It downloads the source code of this website from GitHub and extracts it into Steepâ€™s output directory.\nSteep is able to automatically detect the dependency between the two actions based on their inputs and outputs. The output of the download action is stored in the variable o1, which is in turn used as input for the extract action. This tells Steep to execute the download action first, wait for it to finish, and then execute the extract action.\ndownload\ndownload\nextract\nextract\no1\no1\noutput_directory\noutput_dir...\nhttps://github.com/...\nhttps://gi..."},{"slug":"submit-sequential-workflow","body":"Run the following command on your terminal to submit the new workflow to Steep:\nTerminal\nSimilar to the first tutorial, Steep will return a workflow ID.\nWait until the workflow has finished. If it was successful, you will find the extracted archive in Steepâ€™s output directory:\nTerminal\nSince we did not specify the store flag for the download action, the downloaded .zip file will be stored in Steepâ€™s temporary directory, which is /tmp by default:\nTerminal\nAgain, replace [WORKFLOW ID] with the ID returned by Steep."},{"slug":"sequential-workflows","body":"Tutorials\nThis tutorial teaches you how to create workflows that run two or more services in sequence. It assumes that youâ€™ve familiarised yourself with the basic concepts of Steep and its core data models (workflows and service metadata), and that youâ€™ve already submitted your first workflow.\nYour first workflow\nParallelization\n"},{"slug":"runtime-environments","body":"Steep provides a set of default runtime environments that define how processing services are executed. More environments can be added through runtime environment plugins.\nNameDescriptiondocker The service will be executed through Docker. The service metadata attribute path specifies the Docker image to run. The attribute runtimeArgs specifies parameters that should be forwarded to the docker run command. other The service will be executed like a normal executable program (binary or shell script) \n"},{"slug":"service-parameters","body":"This data model describes the parameters that can be passed to a processing service.\nPropertyTypeDescriptionid\n(required) string A unique parameter identifier name\n(required) string A human-readable name description\n(required) string A human-readable description type\n(required) string The type of this parameter. Valid values: input, output cardinality\n(required) string A string in the form lower..upper specifying how many times the parameter must appear at least (lower limit) and how many times it can appear at most (upper limit). The character n can be used for the upper limit to specify an arbitrary number. The lower limit must not be greater that the upper limit. Examples cardinalities are listed below. dataType\n(optional) string The type of the parameter value. Steep treats parameters differently depending on the data type (see description below). default\n(optional) string An optional default value for this parameter that will be used if the lower limit of cardinality is 1 but no parameter value is given in the workflow. fileSuffix\n(optional) string An optional suffix that should be appended to the generated filename of an output parameter. This property is typically used for file extensions (including the dot), e.g. \".xml\" or \".json\". label\n(optional) string An optional string that will be used as a label for the parameter in the service call. Examples are -i, --input, --resolution, etc. \nEXAMPLE CARDINALITIES:\nâ€¢ \"0..1\" means the parameter is optional (it can appear 0 times or 1 time)\nâ€¢ \"1..1\" means the parameter is mandatory (it must appear 1 time)\nâ€¢ \"1..n\" means it must appear at least once or many times (no upper limit)\nDATA TYPE:\nSteep treats parameters differently depending on the type and dataType:\nâ€¢ If type is \"output\" and dataType is \"directory\", Steep will create a new directory for the serviceâ€™s output and recursively search it for result files after the service has been executed.\nâ€¢ If type is \"input\" and dataType is \"directory\", Steep will find the common parent directory of the files from the parameterâ€™s value and pass it to the service. For example, if the parameterâ€™s value is an array with the elements [\"/tmp/a.txt\", \"/tmp/b.txt\", \"/tmp/subdir/c.txt\"], Steep will pass \"/tmp/\" to the service.\nâ€¢ If type is \"input\", dataType is not \"directory\", but the parameterâ€™s value is an array, Steep will duplicate the parameter as many times as there are items in the array (given that the cardinality has no upper limit).\nâ€¢ If type is \"input\", dataType is \"boolean\", and the parameter has a label, Steep will pass the label to the service if the parameterâ€™s value is true and ignore the parameter if the value is false.\nâ€¢ Otherwise, this property can be an arbitrary string. New data types with special handling can be added through output adapter plugins.\n"},{"slug":"runtime-arguments","body":"Runtime arguments are similar to service parameters, except they are passed to the runtime that executes the service (e.g. Docker) instead of the service itself.\nPropertyTypeDescriptionid\n(required) string A unique argument identifier name\n(required) string A human-readable name description\n(required) string A human-readable description dataType\n(optional) string The type of the parameter value. Typically \"string\" or \"boolean\". The same rules apply as for service parameters. label\n(optional) string An optional string that will be used as a label for the parameter. Examples are -v, --volume, --entrypoint, etc. value\n(optional) string An optional value for this parameter. \n"},{"slug":"service-metadata","body":"Data models\nService metadata is used to describe the interface of a processing service so it can be executed by Steep.\nPropertyTypeDescriptionid\n(required) string A unique service identifier name\n(required) string A human-readable name description\n(required) string A human-readable description path\n(required) string Relative path to the service executable in the service artefact (or a Docker image if runtime equals docker) runtime\n(required) string The runtime environment parameters\n(required) array A list of service parameters runtimeArgs\n(optional) array An optional list of arguments to pass to the runtime requiredCapabilities\n(optional) array A set of strings specifying capabilities a host system must provide to be able to execute this service. See also setups. retries\n(optional) object An optional retry policy specifying how often the execution of this service should be retried in case of an error. Can be overridden in an executable action. maxInactivity\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of this service can take without producing any output (i.e. without writing anything to the standard output and error streams) before it is automatically cancelled or aborted. Can be overridden in an executable action and can be combined with maxRuntime and deadline (see below). Note that a service cancelled due to inactivity is still subject to any configured retry policy, which means its execution may be retried even if one attempt timed out. If you want to cancel a long-running service immediately even if there is a retry policy configured, use a deadline. maxRuntime\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of this service can take before it is automatically cancelled or aborted, even if the service regularly writes to the standard output and error streams. Can be overridden in an executable action and can be combined with maxInactivity (see above) and deadline (see below). Note that a service cancelled due to a too long runtime is still subject to any configured retry policy, which means its execution may be retried even if one attempt timed out. If you want to cancel a long-running service immediately even if there is a retry policy configured, use a deadline. deadline\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of this service can take at all (including all retries and their associated delays) until it is cancelled or aborted. Can be overridden in an executable action and can be combined with maxInactivity and maxRuntime (see above). \nSubmissions\nMacros\n"},{"slug":"servicesservicesyaml","body":"Configuration\nThe file services/services.yaml contains an array of service metadata objects describing the interfaces of all processing services Steep can execute.\nNote: The path to this file can be configured with the item steep.services in Steepâ€™s general configuration.\nservices/services.yaml\nsetups.yaml\nmacros/macros.yaml\n"},{"slug":"volumes","body":"This data model describes an additional volume that can be attached to a virtual machine specified by a setup.\nPropertyTypeDescriptionsizeGb\n(required) number The volumeâ€™s size in gigabytes type\n(optional) string Type the volumeâ€™s type. By default, the type will be selected automatically. availabilityZone\n(optional) string The availability zone in which to create the volume. By default, it will be created in the same availability zone as the VM to which it will be attached. \n"},{"slug":"creation-policies","body":"A creation policy defines rules for creating VMs from a certain setup.\nPropertyTypeDescriptionretries\n(optional) object An optional retry policy that specifies how many attempts should be made to create a VM (if creation fails) as well as possible (exponential) delays between those attempts. If this property is null, default values from the steep.yaml file will be used. lockAfterRetries\n(optional) duration When the maximum number of attempts to create a VM from a certain setup has been reached, the setup will be locked and no other VM with this setup will be created. This property defines how long it will stay locked. \n"},{"slug":"setups","body":"Data models\nA setup describes how a virtual machine (VM) should be created by Steepâ€™s cloud manager.\nPropertyTypeDescriptionid\n(required) string A unique setup identifier flavor\n(required) string The flavor of the new VM imageName\n(required) string The name of the VM image to deploy availabilityZone\n(required) string The availability zone in which to create the VM blockDeviceSizeGb\n(required) number The size of the VMâ€™s main block device in gigabytes blockDeviceVolumeType\n(optional) string An optional type of the VMâ€™s main block device. By default, the type will be selected automatically. minVMs\n(optional) number An optional minimum number of VMs to create with this setup. The default value is 0. maxVMs\n(required) number The maximum number of VMs to create with this setup maxCreateConcurrent\n(optional) number The maximum number of VMs to create and provision concurrently. The default value is 1. provisioningScripts\n(optional) array An optional list of paths to provisioning scripts that should be executed on the VM after it has been created providedCapabilities\n(optional) array An optional set of strings specifying capabilities that VMs with this setup will have sshUsername\n(optional) string An optional username for the SSH connection to the created VM. Overrides the main configuration item steep.cloud.ssh.username if it is defined. additionalVolumes\n(optional) array An optional list of volumes that will be attached to the VM parameters\n(optional) object An optional custom object with user-defined properties. Use this object to pass arbitrary values to provisioning scripts where they can be accessed through the global variable setup. creation\n(optional) object An optional creation policy that defines rules for creating VMs from this setup. Default values for this parameter are defined in the main configuration. \nVMs\nFull-text search\n"},{"slug":"setupsyaml","body":"Configuration\nThe configuration file setups.yaml contains an array of setup objects that Steepâ€™s cloud manager component uses to create new virtual machines and to deploy remote agents to it.\nNote: The path to the setups configuration file can be configured with the item steep.setups in Steepâ€™s general configuration.\nsetups.yaml\nsteep.yaml\nservices/services.yaml\n"},{"slug":"general-configuration","body":"steep.tmpPath: The path to a directory where temporary files should be stored during processing. Steep generates names for the outputs of execute actions in a workflow. If the store flag of an output parameter is false (which is the default), the generated filename will be relative to this temporary directory.\nsteep.outPath: The path to a directory where output files should be stored. This path will be used instead of steep.tmpPath to generate a filename for an output parameter if its store flag is true.\nsteep.overrideConfigFile: The path to a file that keeps additional configuration. The values of the overrideConfigFile will be merged into the main configuration file, so it basically overrides the default values. Note that configuration items in this file can still be overridden with environment variables. This configuration item is useful if you donâ€™t want to change the main configuration file (or if you cannot do so) but still want to set different configuration values. Use it if you run Steep in a Docker container and bind mount the overrideConfigFile as a volume.\nsteep.services: The path to the configuration files containing service metadata. Either a string pointing to a single file, a glob pattern (e.g. **/*.yaml), or an array of files or glob patterns.\nsteep.macros: The path to the configuration file(s) containing macros. Either a string pointing to a single file, a glob pattern (e.g. **/*.yaml), or an array of files or glob patterns.\nsteep.plugins: The path to the configuration file(s) containing plugin descriptors. Either a string pointing to a single file, a glob pattern (e.g. **/*.yaml), or an array of files or glob patterns.\n"},{"slug":"cluster-settings","body":"Use these configuration items to build up a cluster of Steep instances. Under the hood, Steep uses Vert.x and Hazelcast, so these configuration items are very similar to the ones found in these two frameworks. To build up a cluster, you need to configure an event bus connection and a cluster connection. They should use different ports. host typically refers to the machine your instance is running on and publicHost or publicAddress specify the hostname or IP address that your Steep instance will use in your network to advertise itself so that other instances can connect to it.\nFor more information, please read the documentation of Vert.x and Hazelcast.\nsteep.cluster.eventBus.host: \nThe IP address (or hostname) to bind the clustered eventbus to\nDefault: Automatically detected local network interface\nsteep.cluster.eventBus.port: \nThe port the clustered eventbus should listen on\nDefault: A random port\nsteep.cluster.eventBus.publicHost: \nThe IP address (or hostname) the eventbus uses to announce itself within in the cluster\nDefault: Same as steep.cluster.eventBus.host\nsteep.cluster.eventBus.publicPort: \nThe port that the eventbus uses to announce itself within in the cluster\nDefault: Same as steep.cluster.eventBus.port\nsteep.cluster.hazelcast.clusterName: \nAn optional cluster name that can be used to separate clusters of Steep instances. Two instances from different clusters (with different names) cannot connect to each other.\nBy default, no cluster name is set, which means all instances can connect to each other. However, a Steep instance without a cluster name cannot connect to a named cluster.\nHeads up: if you have a cluster name set and youâ€™re using a cloud connection to deploy remote agents on demand, make sure these Steep instances use the same cluster name. Otherwise, you wonâ€™t be able to connect to them.\nsteep.cluster.hazelcast.publicAddress: \nThe IP address (or hostname) and port Hazelcast uses to announce itself within in the cluster\nsteep.cluster.hazelcast.port: \nThe port that Hazelcast should listen on\nsteep.cluster.hazelcast.interfaces: \nA list of IP address patterns specifying valid interfaces Hazelcast should bind to\nsteep.cluster.hazelcast.members: \nA list of IP addresses (or hostnames) of Hazelcast cluster members\nsteep.cluster.hazelcast.tcpEnabled: \ntrue if Hazelcast should use TCP to connect to other instances, false if it should use multicast\nDefault: false\nsteep.cluster.hazelcast.placementGroupName: \nAn optional name specifying in which group this Hazelcast member should be placed. Steep uses distributed maps to share data between instances. Data in these maps is partitioned (i.e. distributed to the individual cluster members). In a large cluster, no member keeps all the data. Most nodes only keep a small fraction of the data (a partition).\nTo make sure data is not lost if a member goes down, Hazelcast uses backups to distribute copies of the data across the cluster. By specifying a placement group, you can control how Hazelcast distributes these backups. Hazelcast will always prefer creating backups in a group that does not own the data so that if all members of a group go down, the other group still has all the backup data.\nExamples for sensible groups are racks, data centers, or availability zones.\nFor more information, see the following links:\nâ€¢ https://docs.hazelcast.com/hazelcast/5.1/architecture/data-partitioning\nâ€¢ https://docs.hazelcast.com/hazelcast/5.1/clusters/partition-group-configuration\nâ€¢ https://docs.hazelcast.com/hazelcast/5.1/data-structures/backing-up-maps\nNote that if you configure a placement group name, all members in your cluster must also have a placement group name. Otherwise, you will receive an exception about mismatching configuration on startup.\nsteep.cluster.hazelcast.liteMember: \ntrue if this instance should be a Hazelcast lite member. Lite members do not own any in-memory data. They are mainly used for compute-intensive tasks. With regard to Steep, an instance with a controller and a scheduler should not be a lite member, because these components heavily rely on internal state. A Steep instance that only contains an agent and therefore only executes services, however, could be a lite member. See the architecture section for more information about these components.\nYour cluster cannot consist of only lite members. Otherwise, it is not able to maintain internal state at all.\nNote that since lite members cannot keep data, they are not suitable to keep backups either. See steep.cluster.hazelcast.placementGroupName for more information. For reasons of reliability, a cluster should contain at least three full (i.e. non-lite) members.\nsteep.cluster.lookupOrphansInterval: \nThe interval at which Steepâ€™s main thread looks for orphaned entries in its internal remote agent registry (specified as a duration). Such entries may (very rarely) happen if there is a network failure during deregistration of an agent. You normally do not have to change this configuration.\nDefault: 5m\nsteep.cluster.hazelcast.restoreMembersOnStartup.enabled: \ntrue if Steep should try to load IP addresses of possibly still running VMs from its database during startup and add them to steep.cluster.hazelcast.members. This is useful if a Steep instance has crashed and should be reintegrated into an existing cluster when itâ€™s back.\nDefault: false\nsteep.cluster.hazelcast.restoreMembersOnStartup.defaultPort: \nIf steep.cluster.hazelcast.restoreMembersOnStartup.enabled is true, potential Hazelcast cluster members will be restored from database. This configuration item specifies on which Hazelcast port these members are listening.\nsteep.cluster.hazelcast.splitBrainProtection.enabled: \ntrue if split-brain protection should be enabled. This mechanism makes sure the cluster is only able to operate if there are at least n members, where n is defined by steep.cluster.hazelcast.splitBrainProtection.minClusterSize. If there are less than n members, Steep instances in the cluster will not be able to access cluster-wide data structures and stop to operate until the issue has been resolved.\nThis mechanism protects against so-called split-brain situations where one part of the cluster loses connection to another part, and the cluster is therefore split into different partitions. If one partition becomes too small, it should stop operating to avoid doing any harm.\nSee the Hazelcast documentation for more information.\nDefault: false\nsteep.cluster.hazelcast.splitBrainProtection.minClusterSize: \nThe minimum number of members the cluster must have to be able operate if split-brain protection is enabled.\nRecommendation: Your cluster should have an odd number of members. The minimum cluster size should be even and represent the majority of your cluster. For example, if your cluster has 7 nodes, set this value to 4. This makes sure that when a split-brain situation happens, the majority of your cluster will be able to continue operating while the smaller part will stop.\nThis configuration item does not have a default value. It must be set if steep.cluster.hazelcast.splitBrainProtection.enable equals true.\nsteep.cluster.hazelcast.splitBrainProtection.gracefulStartup: \ntrue if the split-brain protection mechanism should only start to be in effect once the cluster has reached its minimum size. This allows the cluster to startup gracefully even if the member count is temporarily lower than the defined minimum.\nDefault: true\nsteep.cluster.hazelcast.splitBrainProtection.exitProcessAfter: \nAn optional timeout (specified as a duration) defining how long a Steep instance may keep running after a split-brain situation has been detected. When the timeout is reached and the split-brain situation has not been resolved in the meantime, the Steep instance shuts itself down with exit code 16. This mechanism can be used to prevent a Steep instance from doing any harm when it is in a split-brain situation.\n"},{"slug":"http-configuration","body":"steep.http.enabled: \ntrue if the HTTP interface should be enabled\nDefault: true\nsteep.http.host: \nThe host to bind the HTTP server to\nDefault: localhost\nsteep.http.port: \nThe port the HTTP server should listen on\nDefault: 8080\nsteep.http.postMaxSize: \nThe maximum size of HTTP POST bodies in bytes\nDefault: 1048576 (1 MB)\nsteep.http.basePath: \nThe path where the HTTP endpoints and the web-based user interface should be mounted\nDefault: \"\" (empty string, i.e. no base path)\nsteep.http.allowRoutes: \nA regular expression specifying a whitelist of enabled HTTP routes. Non-matching routes will be disabled. For example, the expression /processchains.* enables all endpoints starting with /processchains but disables all others.\nDefault: .* (all routes are enabled)\nsteep.http.cors.enable: \ntrue if Cross-Origin Resource Sharing (CORS) should be enabled\nDefault: false\nsteep.http.cors.allowOrigin: \nA regular expression specifying allowed CORS origins. Use * to allow all origins.\nDefault: \"$.\" (match nothing by default)\nsteep.http.cors.allowCredentials: \ntrue if the Access- Control- Allow- Credentials response header should be returned.\nDefault: false\nsteep.http.cors.allowHeaders: \nA string or an array indicating which header field names can be used in a request.\nsteep.http.cors.allowMethods: \nA string or an array indicating which HTTP methods can be used in a request.\nsteep.http.cors.exposeHeaders: \nA string or an array indicating which headers are safe to expose to the API of a CORS API specification.\nsteep.http.cors.maxAgeSeconds: \nThe number of seconds the results of a preflight request can be cached in a preflight result cache.\n"},{"slug":"controller-configuration","body":"steep.controller.enabled: \ntrue if the controller should be enabled. Set this value to false if your Steep instance does not have access to the shared database.\nDefault: true\nsteep.controller.lookupInterval: \nThe interval at which the controller looks for accepted submissions, specified as a duration.\nDefault: 2s\nsteep.controller.lookupMaxErrors: \nThe maximum number of consecutive errors (e.g. database connection issues) to tolerate when looking up the status of process chains of a running submission. If there are more errors, the submission will be aborted.\nDefault: 5\nsteep.controller.lookupOrphansInterval: \nThe interval at which the controller looks for orphaned running submissions (i.e. submissions that are in the status RUNNING but that are currently not being processed by any controller instance), specified as a duration. If Steep finds such a submission it will try to resume it.\nDefault: 5m\nsteep.controller.lookupOrphansInitialDelay: \nThe time the controller should wait after startup before it looks for orphaned running submissions for the first time (specified as a duration). This property is useful if you want to implement a rolling update from one Steep instance to another.\nDefault: 0s\n"},{"slug":"scheduler-configuration","body":"steep.scheduler.enabled: \ntrue if the scheduler should be enabled. Set this value to false if your Steep instance does not have access to the shared database.\nDefault: true\nsteep.scheduler.lookupInterval: \nThe interval at which the scheduler looks for registered process chains, specified as a duration.\nDefault: 20s\nsteep.scheduler.lookupOrphansInterval: \nThe interval at which the scheduler looks for orphaned running process chains (i.e. process chains that are in the status RUNNING but that are currently not being processed by any scheduler instance), specified as a duration. Note that the scheduler also always looks for orphaned process chains when it detects that another scheduler instance has just left the cluster (regardless of the configured interval).\nDefault: 5m\nsteep.scheduler.lookupOrphansInitialDelay: \nThe time the scheduler should wait after startup before it looks for orphaned running process chains for the first time (specified as a duration). This property is useful if you want to implement a rolling update from one Steep instance to another. Note that the scheduler also looks for orphaned process chains when another scheduler instance has just left the cluster, even if the initial delay has not passed by yet.\nDefault: 0s\n"},{"slug":"agent-configuration","body":"steep.agent.enabled: \ntrue if this Steep instance should be able to execute process chains (i.e. if one or more agents should be deployed)\nDefault: true\nsteep.agent.instances: \nThe number of agents that should be deployed within this Steep instance (i.e. how many executables the Steep instance can run in parallel)\nDefault: 1\nsteep.agent.id: \nUnique identifier for the first agent instance deployed. Subsequent agent instances will have a consecutive number appended to their IDs.\nDefault: (an automatically generated unique ID)\nsteep.agent.capabilities: \nList of capabilities that the agents provide\nDefault: [] (empty list)\nsteep.agent.autoShutdownTimeout: \nThe time any agent instance can remain idle until Steep shuts itself down gracefully (specified as a duration). By default, this value is 0s, which means Steep never shuts itself down.\nDefault: 0s\nsteep.agent.busyTimeout: \nThe time that should pass before an idle agent decides that it is not busy anymore (specified as a duration). Normally, the scheduler allocates an agent, sends it a process chain, and then deallocates it after the process chain execution has finished. This value is important if the scheduler crashes while the process chain is being executed and does not deallocate the agent anymore. In this case, the agent deallocates itself after the configured time has passed.\nDefault: 1m\nsteep.agent.outputLinesToCollect: \nThe number of output lines to collect at most from each executed service (also applies to error output)\nDefault: 100\n"},{"slug":"runtime-settings","body":"steep.runtimes.docker.env: \nAdditional environment variables that will be passed to containers created by the Docker runtime\nExample: [\"key=value\", \"foo=bar\"]\nDefault: [] (empty list)\nsteep.runtimes.docker.volumes: \nAdditional volume mounts to be passed to the Docker runtime\nExample: [\"/data:/data\"]\nDefault: [] (empty list)\n"},{"slug":"database-connection","body":"steep.db.driver: \nThe database driver\nValid values: inmemory, postgresql, mongodb\nDefault: inmemory\nsteep.db.url: \nThe database URL\nsteep.db.username: \nThe database username (only used by the postgresql driver)\nsteep.db.password: \nThe database password (only used by the postgresql driver)\nsteep.db.connectionPool.maxSize: \nThe maximum number of connections to keep open (i.e. to keep in the connection pool)\nsteep.db.connectionPool.maxIdleTime: \nThe maximum time an idle connection should be kept in the connection pool before it is closed\n"},{"slug":"cloud-connection","body":"steep.cloud.enabled: \ntrue if Steep should connect to a cloud to acquire remote agents on demand\nDefault: false\nsteep.cloud.driver: \nDefines which cloud driver to use\nValid values: openstack (see the OpenStack cloud driver for more information)\nsteep.cloud.createdByTag: \nA metadata tag that should be attached to virtual machines to indicate that they have been created by Steep\nsteep.cloud.syncInterval: \nThe time that should pass before the cloud manager synchronizes its internal state with the cloud again, specified as a duration.\nDefault: 2m\nsteep.cloud.keepAliveInterval: \nThe time that should pass before the cloud manager sends keep-alive messages to a minimum of remote agents again (so that they do not shut down themselves), specified as a duration. See minVMs property of the setups data model.\nDefault: 30s\nsteep.cloud.setups.file: \nThe path to the file that describes all available setups. See setups.yaml.\nsteep.cloud.setups.creation.retries: \nA retry policy that specifies how many attempts should be made to create a VM from a certain setup (if creation fails) as well as possible (exponential) delays between those attempts.\nDEFAULT:\nsteep.cloud.setups.lockAfterRetries: \nWhen the maximum number of attempts to create a VM from a certain setup has been reached (see steep.cloud.setups.creation.retries), the setup will be locked and no other VM with this setup will be created. This parameter defines how long it will be locked, specified as a duration.\nDefault: 20m\nsteep.cloud.timeouts.sshReady: \nThe maximum time the cloud manager should try to log in to a new VM via SSH (specified as a duration). The cloud manager will make a login attempt every 2 seconds until it is successful or until the maximum number of seconds have passed, in which case it will destroy the VM.\nDefault: 5m\nsteep.cloud.timeouts.agentReady: \nThe maximum time the cloud manager should wait for an agent on a new VM to become available (i.e. how long a new Steep instance may take to register with the cluster) before it destroys the VM again (specified as a duration).\nDefault: 5m\nsteep.cloud.timeouts.createVM: \nThe maximum time that creating a VM may take before it is aborted with an error (specified as a duration).\nDefault: 5m\nsteep.cloud.timeouts.destroyVM: \nThe maximum time that destroying a VM may take before it is aborted with an error (specified as a duration).\nDefault: 5m\nsteep.cloud.timeouts.provisioning: \nThe maximum time each individual provisioning step (i.e. executing a provisioning script or uploading files) may take before it is aborted. Running provisioning commands will be killed after this timeout regardless of whether they are still active or not. This value is specified as a duration.\nDefault: 10m\nsteep.cloud.agentPool: \nAn array of agent pool parameters describing how many remote agents the cloud manager should keep in its pool how many it is allowed to create for each given set of capabilities.\nDefault: [] (empty list)\n"},{"slug":"openstack-cloud-driver","body":"steep.cloud.openstack.endpoint: \nOpenStack authentication endpoint\nsteep.cloud.openstack.username: \nOpenStack username used for authentication\nsteep.cloud.openstack.password: \nOpenStack password used for authentication\nsteep.cloud.openstack.domainName: \nOpenStack domain name used for authentication\nsteep.cloud.openstack.projectId: \nThe ID of the OpenStack project to which to connect. Either this configuration item or steep.cloud.openstack.projectName must be set but not both at the same time.\nsteep.cloud.openstack.projectName: \nThe name of the OpenStack project to which to connect. This configuration item will be used in combination with steep.cloud.openstack.domainName if steep.cloud.openstack.projectId is not set.\nsteep.cloud.openstack.networkId: \nThe ID of the OpenStack network to attach new VMs to\nsteep.cloud.openstack.usePublicIp: \ntrue if new VMs should have a public IP address\nDefault: false\nsteep.cloud.openstack.securityGroups: \nThe OpenStack security groups that should be attached to new VMs.\nDefault: [] (empty list)\nsteep.cloud.openstack.keypairName: \nThe name of the keypair to deploy to new VMs. The keypair must already exist in OpenStack.\n"},{"slug":"ssh-connection-to-vms","body":"steep.cloud.ssh.username: \nUsername for SSH access to VMs. Can be overridden by the sshUsername property in each setup. May even be null if all setups define their own username.\nsteep.cloud.ssh.privateKeyLocation: \nLocation of a private key to use for SSH\n"},{"slug":"log-configuration","body":"steep.logs.level: \nThe default log level for all loggers (console as well as file-based)\nValid values: TRACE, DEBUG, INFO, WARN, ERROR, OFF.\nDefault: DEBUG\nsteep.logs.main.enabled: \ntrue if logging to the main log file should be enabled\nDefault: false\nsteep.logs.main.logFile: \nThe name of the main log file\nDefault: logs/steep.log\nsteep.logs.main.dailyRollover.enabled: \ntrue if main log files should be renamed every day. The file name will be based on steep.logs.main.logFile and the fileâ€™s date in the form YYYY-MM-DD (e.g. steep.2020-11-19.log)\nDefault: true\nsteep.logs.main.dailyRollover.maxDays: \nThe maximum number of daysâ€™ worth of main log files to keep\nDefault: 7\nsteep.logs.main.dailyRollover.maxSize: \nThe total maximum size of all main log files in bytes. Oldest log files will deleted when this size is reached.\nDefault: 104857600 (100 MB)\nsteep.logs.processChains.enabled: \ntrue if the output of process chains should be logged separately to disk. The output will still also appear on the console and in the main log file (if enabled), but there, itâ€™s not separated by process chain. This feature is useful if you want to record the output of individual process chains and make it available through the process chain logs endpoint.\nDefault: false\nsteep.logs.processChains.path: \nThe path where process chain logs will be stored. Individual files will will be named after the ID of the corresponding process chain (e.g. aprsqz6d5f4aiwsdzbsq.log). If a process chain has been executed more than once (for example, due to a retry), the file name will include the run number (e.g. aprsqz6d5f4aiwsdzbsq.2.log).\nDefault: logs/processchains\nsteep.logs.processChains.groupByPrefix: \nSet this configuration item to a value greater than 0 to group process chain log files by prefix in subdirectories under the directory configured through steep.logs.processChains.path. For example, if this configuration item is set to 3, Steep will create a separate subdirectory for all process chains whose ID starts with the same three characters. The name of this subdirectory will be these three characters. The process chains apomaokjbk3dmqovemwa and apomaokjbk3dmqovemsq will be put into a subdirectory called apo, and the process chain ao344a53oyoqwhdelmna will be put into ao3. Note that in practice, 3 is a reasonable value, which will create a new directory about every day. A value of 0 disables grouping.\nDefault: 0\n"},{"slug":"garbage-collector-configuration","body":"steep.garbageCollector.enabled: \ntrue if the garbage collector should be enabled. The garbage collector runs in the background and removes outdated objects from the database at the interval specified with steep.garbageCollector.cron\nDefault: false\nsteep.garbageCollector.cron: \nA UNIX-like cron expression specifying the interval at which the garbage collector should be executed. Cron expressions consist of six required fields and one optional field separated by a white space:\nSECONDS MINUTES HOURS DAY-OF-MONTH MONTH DAY-OF-WEEK [YEAR].\nUse an asterisk * to specify all values (e.g. every second or every minute). Use a question mark ? for DAY-OF-MONTH or DAY-OF-WEEK to specify no value (only one of DAY-OF-MONTH or DAY-OF-WEEK can be specified at the same time). Use a slash / to specify increments (e.g. */5 for every 5 minutes).\nMore information about the format can be found in the javadoc of the org.quartz.CronExpression class.\nExample: 0 0 0 * * ? (daily at 12am)\nsteep.garbageCollector.retention.submissions: \nThe maximum time a submission should be kept in the database after it has finished (regardless of whether it was successful or not). The time can be specified as a human-readable duration.\nDefault: null (submissions will be kept indefinitely)\nsteep.garbageCollector.retention.vms: \nThe maximum time a VM should be kept in the database after it has been destroyed (regardless of its status). The time can be specified as a human-readable duration.\nDefault: null (VMs will be kept indefinitely)\n"},{"slug":"cache-configuration","body":"steep.cache.plugins.enabled: \ntrue if the persistent compiled plugin cache should be enabled. Steep updates this cache on startup when it has first compiled a plugin script or when it detects that a previously compiled script has changed. On subsequent startups, Steep can utilize the cache to skip compilation of known plugins and, therefore, to reduce startup time.\nDefault: false\nsteep.cache.plugins.path: \nThe path to a directory where Steep should store compiled plugin scripts if the persistent compiled plugin cache is enabled (see steep.cache.plugins.enabled).\nDefault: .cache/plugins\n"},{"slug":"agent-pool-parameters","body":"Steepâ€™s cloud manager component is able to create virtual machines and deploy remote agent instances to it. The cloud manager keeps every remote agent created in a pool. Use agent pool parameters to define a minimum and maximum number of instances per provided capability set.\nPropertyTypeDescriptioncapabilities\n(required) array A set of strings specÂ­iÂ­fyÂ­ing caÂ­paÂ­bilÂ­iÂ­ties that a remote agent must provide so these parameters apply to it min\n(optional) number An optional minimum number of remote agents that the cloud manager should create with the given capabilities max\n(optional) number An optional maximum number of remote agents that the cloud manager is allowed to create with the given capabilities \n"},{"slug":"steepyaml","body":"Configuration\nThe file steep.yaml contains the main configuration of Steep. This page describes all configuration keys and values you can set.\nNote that keys are specified using the dot notation. You can use them as they are given here or use YAML notation instead. For example, the following configuration item\nis identical to:\nYou may override items in your configuration file with environment variables. This is particularly useful if you are using Steep inside a Docker container. The environment variables use a slightly different naming scheme. All variables are in capital letters and dots are replaced by underscores. For example, the configuration key steep.http.host becomes STEEP_HTTP_HOST and steep.cluster.eventBus.publicPort becomes STEEP_CLUSTER_EVENTBUS_PUBLICPORT. You may use YAML syntax to specify environment variable values. For example, the array steep.agent.capabilities can be specified as follows:\nWeb-based user interface\nsetups.yaml\n"},{"slug":"submission-status","body":"The following table shows the statuses a submission can have:\nStatusDescriptionACCEPTED The submission has been accepted by Steep but execution has not started yet RUNNING The submission is currently being executed CANCELLED The submission was cancelled SUCCESS The execution of the submission finished successfully PARTIAL_SUCCESS The submission was executed completely but one or more process chains failed ERROR The execution of the submission failed \n"},{"slug":"submissions","body":"Data models\nA submission is created when you submit a workflow through the /workflows endpoint. It contains information about the workflow execution such as the start and end time as well as the current status.\nPropertyTypeDescriptionid\n(required) string Unique submission identifier name\n(optional) string An optional human-readable submission name. The value is derived from the submitted workflowâ€™s name if it has any. workflow\n(required) object The submitted workflow priority\n(optional) number A priority used during scheduling. Process chains generated from submissions with higher priorities will be scheduled before those with lower priorities. Negative values are allowed. The value is derived from the submitted workflow but can be overridden. The submissionâ€™s priority always takes precedence over the workflowâ€™s priority when generating process chains. startTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the workflow execution was started. May be null if the execution has not started yet. endTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the workflow execution finished. May be null if the execution has not finished yet. status\n(required) string The current status of the submission requiredCapabilities\n(required) array A set of strings specifying capabilities a host system must provide to be able to execute this workflow. See also setups. source\n(optional) string The original, unaltered workflow source as it was submitted through the /workflows endpoint. May be null if the workflow was not submitted through the endpoint or if the source is unavailable. runningProcessChains\n(required) number The number of process chains currently being executed pausedProcessChains\n(required) number The number of process chains whose execution is currently paused cancelledProcessChains\n(required) number The number of process chains that have been cancelled succeededProcessChains\n(required) number The number of process chains that have finished successfully failedProcessChains\n(required) number The number of process chains whose execution has failed totalProcessChains\n(required) number The current total number of process chains in this submission. May increase during execution when new process chains are generated. results\n(optional) object If status is SUCCESS or PARTIAL_SUCCESS, this property contains the list of workflow result files grouped by their output variable ID. Otherwise, it is null. errorMessage\n(optional) string If status is ERROR, this property contains a human-readable error message. Otherwise, it is null. \nProcess chains\nService metadata\n"},{"slug":"timeout-policies","body":"A timeout policy defines how long a service or an executable may run before it is automatically cancelled or aborted with an error. Timeout policies can be specified with the maxInactivity, maxRuntime and deadline attributes, either per service in the service metadata or per executable action in the workflow.\nA timeout policy is either a string or an object. If it is a string, it represents a duration specifying a maximum amount of time until the execution is cancelled.\nIf specified as an object, the timeout policy has the following properties:\nPropertyTypeDescriptiontimeout duration The maximum amount of time that may pass until the execution is cancelled or aborted. errorOnTimeout\n(optional) boolean true if an execution that is aborted due to a timeout should lead to an error (i.e. if the process chainâ€™s status should be set to ERROR). false if it should just be cancelled (process chain status CANCELLED). By default, the execution will be cancelled. \nMultiple timeout policies can be combined. For example, a service may be cancelled after 5 minutes of inactivity and aborted with an error if its total execution takes longer than 1 hour.\n"},{"slug":"retry-policies","body":"A retry policy specifies how often the execution of a workflow action should be retried in case of an error. Retry policies can be specified per service in the service metadata or per executable action in the workflow.\nPropertyTypeDescriptionmaxAttempts\n(optional) number The maximum number of attempts to perform. This includes the initial attempt. For example, a value of 3 means 1 initial attempt and 2 retries. The default value is 1. A value of -1 means an unlimited (infinite) number of attempts. 0 means there will be no attempt at all (the service or action will be skipped). delay\n(optional) duration The amount of time that should pass between two attempts. The default is 0, which means the operation will be retried immediately. exponentialBackoff\n(optional) number A factor for an exponential backoff (see description below) maxDelay\n(optional) duration The maximum amount of time that should pass between two attempts. Only applies if exponentialBackoff is larger than 1. By default, there is no upper limit. \nEXPONENTIAL BACKOFF:\nThe exponential backoff factor can be used to gradually increase the delay. The actual delay between two attempts will be calculated as follows:\nFor example, if delay equals 1s, exponentialBackoff equals 2, and maxDelay equals 10s, the following actual delays will apply:\nâ€¢ Delay after attempt 1:\n  \n  min(1s * pow(2, 0), 10s) = 1s\nâ€¢ Delay after attempt 2:\n  \n  min(1s * pow(2, 1), 10s) = 2s\nâ€¢ Delay after attempt 3:\n  \n  min(1s * pow(2, 2), 10s) = 4s\nâ€¢ Delay after attempt 4:\n  \n  min(1s * pow(2, 3), 10s) = 8s\nâ€¢ Delay after attempt 5:\n  \n  min(1s * pow(2, 4), 10s) = 10s\nâ€¢ Delay after attempt 6:\n  \n  min(1s * pow(2, 4), 10s) = 10s\nThe default value is 1, which means there is no backoff and the actual delay always equals the specified one.\n"},{"slug":"durations","body":"A duration consists of one or more number/unit pairs possibly separated by whitespace characters. Supported units are:\nâ€¢ milliseconds, millisecond, millis, milli, ms\nâ€¢ seconds, second, secs, sec, s\nâ€¢ minutes, minute, mins, min, m\nâ€¢ hours, hour, hrs, hr, h\nâ€¢ days, day, d\nNumbers must be positive integers. The default unit is milliseconds\nExamples:\n"},{"slug":"timeouts-and-retries","body":"Data models\nThis page describes data models that control how long a workflow execution may take (timeout policies) and what should happen if it runs into an error (retry policies).\nTime-based values in Steepâ€™s data models are specified as human-readable durations.\nMacros\nAgents\n"},{"slug":"using-a-template-engine","body":"Advanced configuration topics\nSteep contains a template engine that allows you to dynamically generate content for YAML-based configuration files (steep.yaml, setups.yaml, services/services.yaml, etc.) at startup.\nYou have to explicitly enable this feature by adding front matter to your YAML file and setting the attribute template to true:\nSteep uses Pebble Templates to compile the configuration files. Please refer to their website for a full documentation on the tags, functions, and filters you can use.\nWithin your template, you may access the following variables:\nVariableDescriptionenv A dictionary of environment variables. Use subscript notation ([]) to access its contents. Example: {{ env[\"PATH\"] }} config A dictionary of configuration properties. This variable is not available in steep.yaml (or any override configuration file). Use subscript notation ([]) to access its contents. Example: {{ config[\"steep.cluster.hazelcast.publicAddress\"] }} \nHere is a full example of a setups.yaml file that uses the templating feature to create two setups with the same parameters, an image name from an environment variable, but different availability zones:\nUsing YAML anchors\nOverview\n"},{"slug":"vm-status","body":"The following table shows the statuses a VM can have:\nStatusDescriptionCREATING The VM is currently being created PROVISIONING The VM has been created and is currently being provisioned (i.e. provisioning scripts defined in the VMâ€™s setup are being executed and the Steep agent is being deployed) RUNNING The VM has been created and provisioned successfully. It is currently running and registered as a remote agent. LEFT The remote agent on this VM has left. It will be destroyed eventually. DESTROYING The VM is currently being destroyed DESTROYED The VM has been destroyed ERROR The VM could not be created, provisioned, or it failed otherwise. See the VMâ€™s reason property for more information. \n"},{"slug":"vms","body":"Data models\nThis data model describes virtual machines created by Steepâ€™s cloud manager.\nPropertyTypeDescriptionid\n(required) string A unique VM identifier externalId\n(optional) string An identifier generated by the cloud platform ipAddress\n(optional) string The VMâ€™s IP address setup\n(required) object The setup used to create this VM creationTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the VM was created. This property is null if the VM has not been created yet. agentJoinTime\n(optional) string An ISO 8601 timestamp denoting the date and time when a Steep agent has been deployed to the VM and has joined the cluster. This property is null if the agent has not joined the cluster yet. destructionTime\n(optional) string An ISO 8601 timestamp denoting the date and time when the VM was destroyed. This property is null if the VM has not been destroyed yet. status\n(required) string The status of the VM reason\n(optional) string The reason why the VM has the current status (e.g. an error message if it has the ERROR status or a simple message indicating why it has been DESTROYED) \nAgents\nSetups\n"},{"slug":"using-yaml-anchors","body":"Advanced configuration topics\nConfiguration files for setups (setups.yaml) and service metadata (services/services.yaml) often contain repeated information (e.g. two setups might offer the same capabilities). You can use YAML anchors to simplify your configuration files.\nThe following example shows two services sharing a parameter seconds. The parameter is defined once for the sleep service and then reused in the docker_sleep service through the YAML anchor sleep_seconds.\nProvisioning scripts\nUsing a template engine\n"},{"slug":"web-based-user-interface","body":"Interfaces\nSteep has a web-based user interface that allows you to monitor the execution of running workflows, process chains, agents, and VMS, as well as to browse the database contents.\nStart Steep and visit any of its HTTP endpoints with your web browser to open the user interface.\nhttp://localhost:8080/workflows\nHTTP endpoints\nsteep.yaml\n"},{"slug":"variables","body":"A variable holds a value for inputs and outputs of processing services. It can be defined (inputs) or undefined (outputs). Defined values are immutable. Undefined variables will be assigned a value by Steep during workflow execution.\nVariables are also used to link two services together and to define the data flow in the workflow graph. For example, if the output parameter of a service A refers to a variable V, and the input parameter of service B refers to the same variable, Steep will first execute A to determine the value of V and then execute B.\nPropertyTypeDescriptionid\n(required) string A unique variable identifier value\n(optional) any The variableâ€™s value or null if the variable is undefined \n"},{"slug":"actions","body":"There are three types of actions in a workflow: execute actions, for-each actions, and include actions. They are differentiated by their type attribute.\nAn execute action instructs Steep to execute a certain service with given inputs and outputs.\nPropertyTypeDescriptionid\n(optional) string An optional string uniquely identifying the action within the workflow. If not given, a random identifier will be generated. type\n(required) string The type of the action. Must be execute. service\n(required) string The ID of the service to execute inputs\n(optional) array An array of input parameters outputs\n(optional) array An array of output parameters dependsOn\n(optional) array A list of identifiers of actions this action needs to finish first before it is ready to be executed. Note that Steep is able to identify dependencies between actions itself based on outputs and inputs, so this attribute is normally not needed. However, it may be useful if a preceding action does not have an output parameter or if the depending action does not have an input parameter. Execute actions may depend on other execute actions but also on for-each actions and include actions and vice versa. retries\n(optional) object An optional retry policy specifying how often this action should be retried in case of an error. Overrides any default retry policy defined in the service metadata. maxInactivity\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of the service can take without producing any output (i.e. without writing anything to the standard output and error streams) before it is automatically cancelled or aborted. Can be combined with maxRuntime and deadline (see below). Overrides any default inactivity timeout defined in the service metadata. Note that a service cancelled due to inactivity is still subject to any configured retry policy, which means its execution may be retried even if one attempt timed out. If you want to cancel a long-running service immediately even if there is a retry policy configured, use a deadline. maxRuntime\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of the service can take before it is automatically cancelled or aborted, even if the service regularly writes to the standard output and error streams. Can be combined with maxInactivity (see above) and deadline (see below). Overrides any default maximum runtime defined in the service metadata. Note that a service cancelled due to a too long runtime is still subject to any configured retry policy, which means its execution may be retried even if one attempt timed out. If you want to cancel a long-running service immediately even if there is a retry policy configured, use a deadline. deadline\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of the service can take at all (including all retries and their associated delays) until it is cancelled or aborted. Can be combined with maxInactivity and maxRuntime (see above). Overrides any default deadline defined in the service metadata. \nA for-each action has an input, a list of sub-actions, and an output. It clones the sub-actions as many times as there are items in its input, executes the actions, and then collects the results in the output.\nAlthough the action is called â€˜for-eachâ€™, the execution order of the sub-actions is undefined (i.e. the execution is non-sequential and non-deterministic). Instead, Steep always tries to execute as many sub-actions as possible in parallel.\nFor-each actions may contain execute actions but also nested for-each actions.\nPropertyTypeDescriptionid\n(optional) string An optional string uniquely identifying the action within the workflow. If not given, a random identifier will be generated. type\n(required) string The type of the action. Must be for. input\n(required) string The ID of a variable containing the items to which to apply the sub-actions enumerator\n(required) string The ID of a variable that holds the current value from input for each iteration output\n(optional) string The ID of a variable that will collect output values from all iterations (see yieldToOutput) dependsOn\n(optional) array A list of identifiers of actions this action needs to finish first before it is ready to be executed. Note that Steep is able to identify dependencies between actions itself based on outputs and inputs, so this attribute is normally not needed. However, it may be useful if a preceding action does not have an output parameter or if the depending action does not have an input parameter. For-each actions may depend on execute actions and include actions but also on other for-each actions and vice versa. actions\n(optional) array An array of sub-actions to execute in each iteration yieldToOutput\n(optional) string The ID of a sub-actionâ€™s output variable whose value should be appended to the for-each actionâ€™s output yieldToInput\n(optional) string The ID of a sub-actionâ€™s output variable whose value should be appended to the for-each actionâ€™s input to generate further iterations \nInclude actions can be used to include the actions of a macro at a certain point in a workflow. They can also be used in macros to include other macros.\nNote that include actions are evaluated in a static pre-processing step during workflow parsing. The pre-processor replaces each include action with the list of actions it specifies and takes care of assigning parameter values as well as renaming IDs and variables to avoid naming collisions.\nPropertyTypeDescriptionid\n(optional) string An optional string uniquely identifying the action within the workflow. If not given, a random identifier will be generated. type\n(required) string The type of the action. Must be include. macro\n(required) string The ID of the macro to include. inputs\n(optional) array An array of input parameters. outputs\n(optional) array An array of include output parameters. dependsOn\n(optional) array A list of identifiers of actions this action needs to finish first before it is ready to be executed. Note that Steep is able to identify dependencies between actions itself based on outputs and inputs, so this attribute is normally not needed. However, it may be useful if a preceding action does not have an output parameter or if the depending action does not have an input parameter. Include actions may depend on execute actions and for-each actions but also on other include actions and vice versa. \nThis data model represents inputs and generic parameters of execute actions as well as inputs of include actions.\nPropertyTypeDescriptionid\n(required) string The ID of the parameter as defined in the service metadata or the macro definition var\n(optional) string The ID of a variable that holds the value for this parameter (required if value is not given) value\n(optional) any The parameter value (required if var is not given) \nNote: Either var or value must be given but not both!\nOutput parameters of execute actions have additional properties compared to inputs.\nPropertyTypeDescriptionid\n(required) string The ID of the parameter as defined in the service metadata var\n(required) string The ID of a variable to which Steep will assign the generated name of the output file. This variable can then be used, for example, as an input parameter of a subsequent action. prefix\n(optional) string An optional string to prepend to the generated name of the output file. For example, if Steep generates the name \"name123abc\" and the prefix is \"my/dir/\", the output filename will be \"my/dir/name123abc\". Note that the prefix must end with a slash if you want to create a directory. The output filename will be relative to the configured temporary directory or output directory (depending on the store property). You may even specify an absolute path: if the generated name is \"name456fgh\" and the prefix is \"/absolute/dir/\", the output filename will be \"/absolute/dir/name456fgh\". store\n(optional) boolean If this property is true, Steep will generate an output filename that is relative to the configured output directory instead of the temporary directory. The default value is false. \nThis data model describes output parameters of include actions.\nPropertyTypeDescriptionid\n(required) string The ID of the parameter as defined in the macro definition var\n(required) string The ID of a variable to which Steep will assign the macroâ€™s return value. This variable can then be used, for example, as an input parameter of a subsequent action. \n"},{"slug":"execute-actions","body":"An execute action instructs Steep to execute a certain service with given inputs and outputs.\nPropertyTypeDescriptionid\n(optional) string An optional string uniquely identifying the action within the workflow. If not given, a random identifier will be generated. type\n(required) string The type of the action. Must be execute. service\n(required) string The ID of the service to execute inputs\n(optional) array An array of input parameters outputs\n(optional) array An array of output parameters dependsOn\n(optional) array A list of identifiers of actions this action needs to finish first before it is ready to be executed. Note that Steep is able to identify dependencies between actions itself based on outputs and inputs, so this attribute is normally not needed. However, it may be useful if a preceding action does not have an output parameter or if the depending action does not have an input parameter. Execute actions may depend on other execute actions but also on for-each actions and include actions and vice versa. retries\n(optional) object An optional retry policy specifying how often this action should be retried in case of an error. Overrides any default retry policy defined in the service metadata. maxInactivity\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of the service can take without producing any output (i.e. without writing anything to the standard output and error streams) before it is automatically cancelled or aborted. Can be combined with maxRuntime and deadline (see below). Overrides any default inactivity timeout defined in the service metadata. Note that a service cancelled due to inactivity is still subject to any configured retry policy, which means its execution may be retried even if one attempt timed out. If you want to cancel a long-running service immediately even if there is a retry policy configured, use a deadline. maxRuntime\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of the service can take before it is automatically cancelled or aborted, even if the service regularly writes to the standard output and error streams. Can be combined with maxInactivity (see above) and deadline (see below). Overrides any default maximum runtime defined in the service metadata. Note that a service cancelled due to a too long runtime is still subject to any configured retry policy, which means its execution may be retried even if one attempt timed out. If you want to cancel a long-running service immediately even if there is a retry policy configured, use a deadline. deadline\n(optional) duration or object An optional duration or timeout policy that defines how long the execution of the service can take at all (including all retries and their associated delays) until it is cancelled or aborted. Can be combined with maxInactivity and maxRuntime (see above). Overrides any default deadline defined in the service metadata. \n"},{"slug":"for-each-actions","body":"A for-each action has an input, a list of sub-actions, and an output. It clones the sub-actions as many times as there are items in its input, executes the actions, and then collects the results in the output.\nAlthough the action is called â€˜for-eachâ€™, the execution order of the sub-actions is undefined (i.e. the execution is non-sequential and non-deterministic). Instead, Steep always tries to execute as many sub-actions as possible in parallel.\nFor-each actions may contain execute actions but also nested for-each actions.\nPropertyTypeDescriptionid\n(optional) string An optional string uniquely identifying the action within the workflow. If not given, a random identifier will be generated. type\n(required) string The type of the action. Must be for. input\n(required) string The ID of a variable containing the items to which to apply the sub-actions enumerator\n(required) string The ID of a variable that holds the current value from input for each iteration output\n(optional) string The ID of a variable that will collect output values from all iterations (see yieldToOutput) dependsOn\n(optional) array A list of identifiers of actions this action needs to finish first before it is ready to be executed. Note that Steep is able to identify dependencies between actions itself based on outputs and inputs, so this attribute is normally not needed. However, it may be useful if a preceding action does not have an output parameter or if the depending action does not have an input parameter. For-each actions may depend on execute actions and include actions but also on other for-each actions and vice versa. actions\n(optional) array An array of sub-actions to execute in each iteration yieldToOutput\n(optional) string The ID of a sub-actionâ€™s output variable whose value should be appended to the for-each actionâ€™s output yieldToInput\n(optional) string The ID of a sub-actionâ€™s output variable whose value should be appended to the for-each actionâ€™s input to generate further iterations \n"},{"slug":"include-actions","body":"Include actions can be used to include the actions of a macro at a certain point in a workflow. They can also be used in macros to include other macros.\nNote that include actions are evaluated in a static pre-processing step during workflow parsing. The pre-processor replaces each include action with the list of actions it specifies and takes care of assigning parameter values as well as renaming IDs and variables to avoid naming collisions.\nPropertyTypeDescriptionid\n(optional) string An optional string uniquely identifying the action within the workflow. If not given, a random identifier will be generated. type\n(required) string The type of the action. Must be include. macro\n(required) string The ID of the macro to include. inputs\n(optional) array An array of input parameters. outputs\n(optional) array An array of include output parameters. dependsOn\n(optional) array A list of identifiers of actions this action needs to finish first before it is ready to be executed. Note that Steep is able to identify dependencies between actions itself based on outputs and inputs, so this attribute is normally not needed. However, it may be useful if a preceding action does not have an output parameter or if the depending action does not have an input parameter. Include actions may depend on execute actions and for-each actions but also on other include actions and vice versa. \n"},{"slug":"parameters","body":"This data model represents inputs and generic parameters of execute actions as well as inputs of include actions.\nPropertyTypeDescriptionid\n(required) string The ID of the parameter as defined in the service metadata or the macro definition var\n(optional) string The ID of a variable that holds the value for this parameter (required if value is not given) value\n(optional) any The parameter value (required if var is not given) \nNote: Either var or value must be given but not both!\n"},{"slug":"output-parameters","body":"Output parameters of execute actions have additional properties compared to inputs.\nPropertyTypeDescriptionid\n(required) string The ID of the parameter as defined in the service metadata var\n(required) string The ID of a variable to which Steep will assign the generated name of the output file. This variable can then be used, for example, as an input parameter of a subsequent action. prefix\n(optional) string An optional string to prepend to the generated name of the output file. For example, if Steep generates the name \"name123abc\" and the prefix is \"my/dir/\", the output filename will be \"my/dir/name123abc\". Note that the prefix must end with a slash if you want to create a directory. The output filename will be relative to the configured temporary directory or output directory (depending on the store property). You may even specify an absolute path: if the generated name is \"name456fgh\" and the prefix is \"/absolute/dir/\", the output filename will be \"/absolute/dir/name456fgh\". store\n(optional) boolean If this property is true, Steep will generate an output filename that is relative to the configured output directory instead of the temporary directory. The default value is false. \n"},{"slug":"include-output-parameters","body":"This data model describes output parameters of include actions.\nPropertyTypeDescriptionid\n(required) string The ID of the parameter as defined in the macro definition var\n(required) string The ID of a variable to which Steep will assign the macroâ€™s return value. This variable can then be used, for example, as an input parameter of a subsequent action. \n"},{"slug":"retry-policy-defaults","body":"A default retry policy that should be used within a workflow unless a more specific retry policy is defined elsewhere.\nPropertyTypeDescriptionprocessChains\n(optional) object An optional default retry policy that should be applied to every generated process chain \n"},{"slug":"workflows","body":"Data models\nThe main components of the workflow model are variables and actions. Use variables to specify input files and parameters for your processing services. Variables for output files must not have a value. The names of output files will be generated by Steep during workflow execution.\nPropertyTypeDescriptionapi\n(required) string The API (or data model) version. Should be 4.7.0. name\n(optional) string An optional human-readable workflow name priority\n(optional) number A priority used during scheduling. Process chains generated from workflows with higher priorities will be scheduled before those with lower priorities. Negative values are allowed. The default value is 0. retries\n(optional) object Default retry policies that should be used within the workflow unless more specific retry policies are defined elsewhere. vars\n(optional) array An array of variables actions\n(required) array An array of actions that make up the workflow \nBring your own service\nProcess chains\n"},{"slug":"add-service-metadata","body":"Letâ€™s assume we want to create a workflow that creates a copy of a file. For this, we can use the cp command, which is built-in into all UNIX distributions (under Windows, you can use the copy command instead). Open the file conf/services/services.yaml and add service metadata below.\nThe default distribution of Steep already contains metadata for this service, so you actually donâ€™t need to do anything here.\nconf/services/services.yaml\nThis metadata describes a service with the ID cp. It has a human-readable name and description. The path attribute specifies the path to the serviceâ€™s executable. Since the cp command is built-in, we can just use cp here (replace this with copy under Windows).\nThe service has two parameters, input_file and output_file, which represent the serviceâ€™s input and output parameters, respectively. Both have a cardinality of 1..1, which means they are mandatory. They also have human-readable names and descriptions.\nSteep uses this metadata later during workflow execution to generate a command line in the following form:\n"},{"slug":"restart-steep","body":"Restart Steep if it is running, so it can pick up the updated configuration."},{"slug":"create-the-workflow","body":"Now, letâ€™s create a workflow that uses the new cp service. Create a file copy.yaml anywhere on your computer and add the following content:\ncopy.yaml\nThe workflow consists of one execute action calling our cp service. /path/to/example.txt is the path to the file that should be copied. Replace this with the absolute path to any file on your computer.\nThe name of the output file will be generated later during workflow execution by Steep. It will be stored in the variable o1.\nAny output filename generated is relative to either Steepâ€™s temporary directory (tmpPath) or output directory (outPath), which can be configured in the general configuration file. In our workflow, we set store to true to generate a filename in the output directory."},{"slug":"submit-the-workflow","body":"Itâ€™s now time to submit the workflow to Steep. Run the following command on your terminal:\nTerminal\nSteep will return a submission object with runtime information about the workflow including a unique ID. You can either open the web-based user interface at http://localhost:8080 or monitor the workflow execution on the command line:\nTerminal\nReplace [WORKFLOW ID] with the unique ID returned by Steep after submitting the workflow.\nWhen the workflow has finished, its status will change to SUCCESS. You will find the file copy in the output directory, which is /tmp/steep/out by default. Steep creates a subdirectory for each workflow based in the workflowâ€™s ID:\nTerminal\n"},{"slug":"your-first-workflow","body":"Tutorials\nLearn how to run your first simple workflow. This tutorial assumes that youâ€™ve already downloaded and extracted Steep and that it is running and listens to incoming requests on port 8080.\nA Steep workflow consists of a set of processing services that are executed in a specific order (see How does Steep work?). Every service that should be executed has to be described with so-called service metadata.\nIn order to run a workflow, you therefore need at least two things:\n 1. The workflow itself as a YAML document or JSON file (we prefer YAML for its readability).\n 2. The service metadata for all services that are used in the workflow. This metadata has to be stored in the services configuration file. The default path to this file is conf/services/services.yaml in Steepâ€™s application directory but can be configured.\nHow does Steep work?\nSequential workflows\n"}]